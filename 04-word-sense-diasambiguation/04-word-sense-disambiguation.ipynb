{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (〃￣︶￣)人(〃￣︶￣〃)人(〃￣︶￣〃)人(〃￣︶￣〃)人(￣︶￣〃)\n",
    "      Judit      Minerva       Patri        Ranim       Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation using Neural Networks\n",
    "Adam Ek\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on [how to work on group assignments](https://github.com/sdobnik/computational-semantics/blob/master/README.md).\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with static distributional vectors is the difficulty of distinguishing between different *word senses*. We will continue our exploration of word vectors by considering *trainable vectors* or *word embeddings* for Word Sense Disambiguation (WSD).\n",
    "\n",
    "The goal of word sense disambiguation is to train a model to find the sense of a word (homonyms of a word-form). For example, the word \"bank\" can mean \"sloping land\" or \"financial institution\". \n",
    "\n",
    "(a) \"I deposited my money in the **bank**\" (financial institution)\n",
    "\n",
    "(b) \"I swam from the river **bank**\" (sloping land)\n",
    "\n",
    "In case a) and b) we can determine that the meaning of \"bank\" based on the *context*. To utilize context in a semantic model we use *contextualized word representations*. Previously we worked with *static word representations*, i.e. the representation does not depend on the context. To illustrate we can consider sentences (a) and (b), the word **bank** would have the same static representation in both sentences, which means that it becomes difficult for us to predict its sense. What we want is to create representations that depend on the context, i.e. *contextualized embeddings*. \n",
    "\n",
    "We will create contextualized embeddings with Recurrent Neural Networks. You can read more about recurrent neural netoworks [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Your overall task in this lab is to create a neural network model that can disambiguate the word sense of 15 different words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we import some packages that we need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "import numpy as np\n",
    "# our hyperparameters (add more when/if you need them)\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "epochs = 3\n",
    "max_grad_norm = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working with data\n",
    "\n",
    "A central part of any machine learning system is the data we're working with. In this section we will split the data (the dataset is located here: ``wsd-data/wsd_data.txt``) into a training set and a test set. We will also create a baseline to compare our model against. Finally, we will use TorchText to transform our data (raw text) into a convenient format that our neural network can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset we will use contain different word sense for 15 different words. The data is organized as follows (values separated by tabs): \n",
    "- Column 1: word-sense\n",
    "- Column 2: word-form\n",
    "- Column 3: index of word\n",
    "- Column 4: white-space tokenized context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "Your first task is to seperate the data into a *training set* and a *test set*. The training set should contain 80% of the examples and the test set the remaining 20%. The examples for the test/training set should be selected **randomly**. Save each dataset into a .csv file for loading later. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path = 'wsd-data/wsd_data.txt'\n",
    "\n",
    "def data_split(path_to_dataset):\n",
    "    \n",
    "    with open(path_to_dataset) as d:\n",
    "        di = d.readlines()\n",
    "    \n",
    "        train, test = train_test_split(di, test_size=0.20, random_state=0)\n",
    "\n",
    "        with open('train.csv', 'w') as g:\n",
    "            for l in train:\n",
    "                g.write(l)\n",
    "        with open('test.csv', 'w') as h:\n",
    "            for l in test:\n",
    "                h.write(l)\n",
    "\n",
    "data_split(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a baseline\n",
    "\n",
    "Your second task is to create a *baseline* for the task. A baseline is a \"reality check\" for a model, given a very simple heuristic/algorithmic/model solution to the problem, can our neural network perform better than this?\n",
    "The baseline you are to create is the \"most common sense\" (MCS) baseline. For each word form, find the most commonly assigned sense to the word, and label a words with that sense. **[2 marks]**\n",
    "\n",
    "E.g. In a fictional dataset, \"bank\" have two senses, \"financial institution\" which occur 5 times and \"side of river\" 3 times. Thus, all 8 occurences of bank is labeled \"financial institution\" and this yields an MCS accuracy of 5/8 = 62.5%. If a model obtain a higher score than this, we can conclude that the model *at least* is better than selecting the most frequent word sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcs_baseline(data):\n",
    "    result = {}\n",
    "    with open(data) as file:\n",
    "        for line in file.readlines():\n",
    "            split_line = line.split(\"\\t\")\n",
    "            if split_line[1] in result:\n",
    "                if split_line[0] in result[split_line[1]]:\n",
    "                    result[split_line[1]][split_line[0]] += 1\n",
    "                else:\n",
    "                    result[split_line[1]][split_line[0]] = 1\n",
    "            else:\n",
    "                result[split_line[1]] = {split_line[0]: 1}\n",
    "    \n",
    "    # select most common sense for words\n",
    "    for key, value in result.items():\n",
    "        result[key] = max(value, key=value.get)\n",
    "        # print(key, len(value.values()))\n",
    "        \n",
    "    return result\n",
    "\n",
    "baseline = mcs_baseline(\"train.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data iterators\n",
    "\n",
    "To train a neural network, we first need to prepare the data. This involves converting words (and labels) to a number, and organizing the data into batches. We also want the ability to shuffle the examples such that they appear in a random order.  \n",
    "\n",
    "To do all of this we will use the torchtext library (https://torchtext.readthedocs.io/en/latest/index.html). In addition to converting our data into numerical form and creating batches, it will generate a word and label vocabulary, and data iterators than can sort and shuffle the examples. \n",
    "\n",
    "Your task is to create a dataloader for the training and test set you created previously. So, how do we go about doing this?\n",
    "\n",
    "1) First we create a ``Field`` for each of our columns. A field is a function which tokenize the input, keep a dictionary of word-to-numbers, and fix paddings. So, we need four fields, one for the word-sense, one for the position, one for the lemma and one for the context. \n",
    "\n",
    "2) After we have our fields, we need to process the data. For this we use the ``TabularDataset`` class. We pass the name and path of the training and test files we created previously, then we assign which field to use in each column. The result is that each column will be processed by the field indicated. So, the context column will be tokenized and processed by the context field and so on. \n",
    "\n",
    "3) After we have processed the dataset we need to build the vocabulary, for this we call the function ``build_vocab()`` on the different ``Fields`` with the output from ``TabularDataset`` as input. This looks at our dataset and creates the necessary vocabularies (word-to-number mappings). \n",
    "\n",
    "4) Finally, the last step. In the last step we load the data objects given by the ``TabularDataset`` and pass it to the ``BucketIterator`` class. This class will organize our examples into batches and shuffle them around (such that for each epoch the model observe the examples in a different order). When we are done with this we can let our function return the data iterators and vocabularies, then we are ready to train and test our model!\n",
    "\n",
    "Implement the dataloader. [**2 marks**]\n",
    "\n",
    "*hint: for TabularDataset and BucketIterator use the class function splits()* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(path):\n",
    "    \n",
    "    whitespacer = lambda x: x.split(' ')\n",
    "\n",
    "    # \"fields\" that process the different columns in our CSV files\n",
    "    WORDSENSE = Field(tokenize    = whitespacer,\n",
    "               lower       = True,\n",
    "               batch_first = True)\n",
    "\n",
    "    LEMMA = Field(tokenize    = whitespacer,\n",
    "                  lower       = True,\n",
    "                  batch_first = True)\n",
    "    \n",
    "    POSITION = Field(tokenize    = whitespacer,\n",
    "                     sequential = False,\n",
    "                     use_vocab = False, # To make sure you don't use the vocabulary for this field\n",
    "                     batch_first = True)\n",
    "    \n",
    "    CONTEXT = Field(tokenize    = whitespacer,\n",
    "                    lower       = True,\n",
    "                    batch_first = True)\n",
    "    \n",
    "    # read the csv files\n",
    "    train, test = TabularDataset.splits(path   = path,\n",
    "                                        train  = 'train.csv',\n",
    "                                        test   = 'test.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = [('sense', WORDSENSE),\n",
    "                                                  ('lemma', LEMMA),\n",
    "                                                 ('position', POSITION),\n",
    "                                                 ('context', CONTEXT)],\n",
    "                                        skip_header       = True,\n",
    "                                        csv_reader_params = {'delimiter':'\\t',\n",
    "                                                             'quotechar':'½'})\n",
    "    \n",
    "    # build vocabularies based on what our csv files contained and create word2id mapping\n",
    "    WORDSENSE.build_vocab(train) #, min_freq=3) \n",
    "    LEMMA.build_vocab(train)\n",
    "    CONTEXT.build_vocab(train)\n",
    "\n",
    "    # create batches from our data, and shuffle them for each epoch\n",
    "    train_iter, test_iter = BucketIterator.splits((train, test),\n",
    "                                                  batch_size        = batch_size,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.lemma),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "\n",
    "    return train_iter, test_iter, WORDSENSE, LEMMA, POSITION, CONTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Creating and running a Neural Network for WSD\n",
    "\n",
    "In this section we will create and run a neural network to predict word senses based on *contextualized representations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We will use a bidirectional Long-Short-Term Memory (LSTM) network to create a representation for the sentences and a Linear classifier to predict the sense of each word.\n",
    "\n",
    "When we initialize the model, we need a few things:\n",
    "\n",
    "    1) An embedding layer: a dictionary from which we can obtain word embeddings\n",
    "    2) A LSTM-module to obtain contextual representations\n",
    "    3) A classifier that compute scores for each word-sense given *some* input\n",
    "\n",
    "\n",
    "The general procedure is the following:\n",
    "\n",
    "    1) For each word in the sentence, obtain word embeddings\n",
    "    2) Run the embedded sentences through the RNN\n",
    "    3) Select the appropriate hidden state\n",
    "    4) Predict the word-sense \n",
    "\n",
    "**Suggestion for efficiency:**  *Use a low dimensionality (32) for word embeddings and the LSTM when developing and testing the code, then scale up when running the full training/tests*\n",
    "    \n",
    "Your tasks will be to create two different models (both follow the two outlines described above), described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first approach to WSD, you are to select the index of our target word (column 3 in the dataset) and predict the word sense. **[5 marks]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam: \"So in the first approach we want to use the LSTM (contextual) representation of the ambiguous word to predict its sense, so we need to **extract that representation and pass it to our prediction layer**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDModel_approach1(nn.Module):\n",
    "    def __init__(self, num_words, num_senses, i_dim, o_dim):\n",
    "        super(WSDModel_approach1, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_words, i_dim) \n",
    "        self.rnn = nn.LSTM(i_dim, o_dim, bidirectional=True, batch_first=True)\n",
    "        self.classifier = nn.Linear(o_dim*2, num_senses) # the output of the lstm (TIMES TWO) is the input of the linnear\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        embedded_batch = self.embeddings(batch.context)\n",
    "        hidden_states, (final_hidden, cell_state) = self.rnn(embedded_batch) # you put the embedding in the context\n",
    "        # hidden_states => multidim matrix of size [batch_size, sequence_lentght, o_dim]\n",
    "        # select by index from hidden_states\n",
    "        word = hidden_states[range(hidden_states.shape[0]), batch.position]\n",
    "        \n",
    "        return self.classifier(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second approach to WSD, you are to predict the word sense based on the final hidden state given by the RNN. **[5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDModel_approach2(nn.Module):\n",
    "    def __init__(self, num_words, num_senses, i_dim, o_dim):\n",
    "        super(WSDModel_approach2, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_words, i_dim)\n",
    "        self.rnn = nn.LSTM(i_dim, o_dim, bidirectional=True, batch_first=True)\n",
    "        self.classifier = nn.Linear(o_dim*2, num_senses)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        embedded_batch = self.embeddings(batch.context) \n",
    "        hidden_states, (final_hidden, cell_state) = self.rnn(embedded_batch)\n",
    "        \n",
    "        not_cool_approach = torch.cat((final_hidden[0], final_hidden[1]), dim=1)\n",
    "        \n",
    "        # cool_approach = torch.cat((final_hidden[0], cell_state[0], final_hidden[1], cell_state[1]), dim=1)\n",
    "        \n",
    "        # concatenate forward and backward OR concatenate final_hidden with cell_state\n",
    "        pred = self.classifier(not_cool_approach)\n",
    "        \n",
    "        # pred = self.classifier(cool_approach)\n",
    "\n",
    "        return pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the model\n",
    "\n",
    "Now we are ready to train and test our model. What we need now is a loss function, an optimizer, and our data. \n",
    "\n",
    "- First, create the loss function and the optimizer.\n",
    "- Next, we iterate over the number of epochs (i.e. how many times we let the model see our data). \n",
    "- For each epoch, iterate over the dataset (``train_iter``) to obtain batches. Use the batch as input to the model, and let the model output scores for the different word senses.\n",
    "- For each model output, calculate the loss (and print the loss) on the output and update the model parameters.\n",
    "- Reset the gradients and repeat.\n",
    "- After all epochs are done, test your trained model on the test set (``test_iter``) and calculate the total and per-word-form accuracy of your model.\n",
    "\n",
    "Implement the training and testing of the model **[4 marks]**\n",
    "\n",
    "**Suggestion for efficiency:** *when developing your model, try training and testing the model on one or two batches (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_iter):\n",
    "\n",
    "    loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "\n",
    "        for i, batch in enumerate(train_iter): \n",
    "            sentences = batch.context\n",
    "            senses    = batch.sense\n",
    "\n",
    "            # run sentences through the model\n",
    "            output = model(batch) \n",
    "\n",
    "            # compute loss\n",
    "            # output: from (B, L, C) to (B*L, C)\n",
    "            # labels: from (B, C) to (B*C)\n",
    "            # where B = batch size, L = sequence length and C the number of labels\n",
    "\n",
    "            batch_loss  = loss(output, senses.view(-1))\n",
    "            epoch_loss += batch_loss.item()\n",
    "\n",
    "            # report results\n",
    "            print(e, (i+1)*sentences.size(0), np.round(epoch_loss/(i+1),4),\n",
    "                  end='\\r')\n",
    "\n",
    "            # calculate gradients\n",
    "            batch_loss.backward()\n",
    "            \n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print()\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_iter):\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "    \n",
    "    model.eval()\n",
    "    # test model after all epochs are completed\n",
    "    test_loss = 0\n",
    "\n",
    "    # iterate over the test data and compute the class probabilities, same\n",
    "    # procedure as before, but now we don't backpropagate\n",
    "    \n",
    "    correct_guesses = 0\n",
    "    \n",
    "    for i, batch in enumerate(test_iter):\n",
    "    #     sentences = batch.context\n",
    "        senses    = batch.sense\n",
    "\n",
    "        with torch.no_grad(): # don't collect gradients when testing\n",
    "            output = model(batch)\n",
    "\n",
    "        batch_loss = loss(output.view(-1,num_senses), senses.view(-1))\n",
    "        test_loss += batch_loss.item()\n",
    "\n",
    "        # finding accuracy\n",
    "        correct_guesses += torch.sum(torch.eq(torch.argmax(output, dim=1), senses.view(-1)).long())\n",
    "    \n",
    "    accuracy = int(correct_guesses) / ((i+1) * batch_size)\n",
    "\n",
    "    print('>', np.round(test_loss/(i+1), 4))\n",
    "    print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of a RuntimeError: The NVIDIA driver on your system is too old (found version 10010),\n",
    "# run the following command: pip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html            \n",
    "\n",
    "path_to_folder = '.'\n",
    "train_iter, test_iter, sense, lemma, position, context = dataloader(path_to_folder)\n",
    "\n",
    "num_words  = len(context.vocab)\n",
    "num_senses = len(sense.vocab)\n",
    "\n",
    "model1 = WSDModel_approach1(num_words, num_senses, 50, 50).to(device) \n",
    "model2 = WSDModel_approach2(num_words, num_senses, 50, 50).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 60840 1.32412.08991.7512 1.3407\n",
      "1 60840 0.868319776 0.88150.87670.87590.872153428 0.8696\n",
      "2 60840 0.71080.7050.7065\n"
     ]
    }
   ],
   "source": [
    "m1 = train(model1, train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 60840 4.8295.12864.959423832 4.95894.8797\n",
      "1 60840 4.0989.48194.42934.3796\n",
      "2 30420 2.7269\n"
     ]
    }
   ],
   "source": [
    "m2 = train(model2, train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0.9117\n",
      "accuracy:  0.6888640546936629\n"
     ]
    }
   ],
   "source": [
    "test_model(m1, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2.2046\n",
      "accuracy:  0.41171443597160134\n"
     ]
    }
   ],
   "source": [
    "test_model(m2, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Running a transformer for WSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the lab you'll try out the transformer, specifically the BERT model. For this we'll use the huggingface library (https://huggingface.co/).\n",
    "\n",
    "You can find the documentation for the BERT model here (https://huggingface.co/transformers/model_doc/bert.html) and a general usage guide here (https://huggingface.co/transformers/quickstart.html).\n",
    "\n",
    "What we're going to do is *fine-tune* the BERT model, i.e. update the weights of a pre-trained model. That is, we have a model that is trained on language modeling, but now we apply it to word sense disambiguation with the word representations it learnt from language modeling.\n",
    "\n",
    "We'll use the same data splits for training and testing as before, but this time you'll not use a torchtext dataloader. Rather now you create an iterator that collects N sentences (where N is the batch size) then use the **BertTokenizer to transform the sentence into integers**. For your dataloader, remember to:\n",
    "* Shuffle the data in each batch\n",
    "* Make sure you get a new iterator for each *epoch*\n",
    "* Create a vocabulary of *sense-labels* so you can calculate accuracy \n",
    "\n",
    "We then pass this batch into the BERT model and train as before. The BERT model will encode the sentence, then we send this encoded sentence into a prediction layer (you can either the the sentence-representation from bert, or the ambiguous word) like before and collect sense predictions.\n",
    "\n",
    "About the hyperparameters and training:\n",
    "* For BERT, usually a lower learning rate works best, between 0.0001-0.000001.\n",
    "* BERT takes alot of resources, running it on CPU will take ages, utilize the GPUs :)\n",
    "* Since BERT takes alot of resources, use a small batch size (4-8)\n",
    "* Computing the BERT representation, make sure you pass the mask\n",
    "\n",
    "**[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.000001 # ┗( T﹏T )┛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def yield_batches(lst, batch_size):\n",
    "    # Shuffle the data\n",
    "    random.shuffle(lst)\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "def dataloader_for_bert(path_to_file, batch_size):\n",
    "    with open(path_to_file) as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        #contexts = [ line.split('\\t')[3] for line in lines]\n",
    "        contexts_and_labels = [ (line.split('\\t')[3], line.split('\\t')[0]) for line in lines]\n",
    "        \n",
    "        #Create a vocabulary of sense-labels so you can calculate accuracy\n",
    "        sense_labels = {line.split('\\t')[0] for line in lines}\n",
    "        \n",
    "        # create batches\n",
    "        iterator = yield_batches(contexts_and_labels, batch_size)\n",
    "        \n",
    "        # use BertTokenizer to encode sentences \n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        context_labels = []\n",
    "        \n",
    "        # read batch_size sentences at a time\n",
    "        for i, batch in enumerate(iterator):\n",
    "            contexts, labels = zip(*batch)\n",
    "            tokenized_text = tokenizer.batch_encode_plus(contexts, \n",
    "                                                        max_length=128,\n",
    "                                                        add_special_tokens = True, # add CLS and SEP tokens\n",
    "                                                        pad_to_max_length=True,\n",
    "                                                        padding = 'longest',\n",
    "                                                        truncation=True,\n",
    "                                                        return_attention_mask=True)\n",
    "            input_ids.append(tokenized_text['input_ids'])\n",
    "            attention_masks.append(tokenized_text['attention_mask'])\n",
    "            # for every batch of tokenized_text we also need to add its labels\n",
    "            context_labels.append(labels)\n",
    "       \n",
    "    return sense_labels, input_ids, attention_masks, context_labels, i #torch.LongTensor(input_ids), torch.FloatTensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dataloader_for_bert(\"train.csv\", batch_size) # (∪.∪ )...zzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = dataloader_for_bert(\"test.csv\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class BERT_WSD(nn.Module):\n",
    "    def __init__(self, sense_labels_size):\n",
    "        super(BERT_WSD, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, sense_labels_size)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        input_ids = torch.LongTensor(batch[0]).to(device)\n",
    "        attention_mask = torch.FloatTensor(batch[1]).to(device)\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False) \n",
    "        # select cls tokens -> the first element of last_hidden_state \n",
    "        predictions = self.classifier(output[0][:,0,:])\n",
    "        #predictions = self.classifier(output[1]) # automatically selects cls tokens\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the Train function\n",
    "Now we are all set to train our model. This train function is just like how we process a normal Pytorch model. We first set the mode to training, then we iterate through each batch and transfer it to the GPU. Then we pass the input_ids, attention_mask and input_ids to the model. It gives us the output, which consists of loss, logits, hidden_states_output and attention_mask_output. The loss contains the classification loss value. We call the backward function of the loss to calculate the gradients of the parameters of the BERT model. We then call clip_grad_norm_ to prevent the gradients from getting too high or too low. Then we call the optimizer.step() to update the gradients which are calculated by loss.backward(). scheduler.step() is used to update the learning rate according to the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "sense_labels, input_ids, attention_masks, context_labels, num_batches = train_dataloader\n",
    "model = BERT_WSD(len(sense_labels)).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer,\n",
    "                                                         num_warmup_steps = int((num_batches * epochs) * 0.05),\n",
    "                                                         num_training_steps = num_batches * epochs)\n",
    "\n",
    "# convert sense_labels to integers\n",
    "sense_labels_dict = {label: i for i, label in enumerate(sense_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9145213935462992\n",
      "2.9122591788207597\n",
      "2.9129524720177535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BERT_WSD. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertIntermediate. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertPooler. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/guskhojra@GU.GU.SE/envs/lib64/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# (╯°□°）╯︵ ┻━┻ *flips table*\n",
    "\n",
    "model.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    # Suggestion for a different approach: get batches here instead\n",
    "    for i, batch_tuple in enumerate(zip(input_ids, attention_masks, context_labels)):\n",
    "        # output from model\n",
    "        out = model(batch_tuple)\n",
    "        \n",
    "        target_labels = torch.tensor([sense_labels_dict[label] for label in batch_tuple[2]]).to(device)\n",
    "        loss  = loss_function(out, target_labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # print average loss for the epoch\n",
    "        print(total_loss/(i+1), end='\\r')\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimizing\n",
    "        optimizer.step()\n",
    "            \n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "    print()\n",
    "torch.save(model, 'bert_model_0.5.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi Ranim, Minerva and Adam! This is Pat. I screwed up and did a keybord interrupt so I had to restart it. I am SO sorry!!! Please forgive me my dudes. (っ °Д °;)っ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"/scratch/bert_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2.7181\n",
      "BERT accuracy:  0.5275440441756508\n"
     ]
    }
   ],
   "source": [
    "sense_labels, input_ids, attention_masks, context_labels, _ = test_dataloader\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct_guesses = 0\n",
    "\n",
    "for i, batch_tuple in enumerate(zip(input_ids, attention_masks, context_labels)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(batch_tuple)\n",
    "\n",
    "    target_labels = torch.tensor([sense_labels_dict[label] for label in batch_tuple[2]]).to(device)\n",
    "\n",
    "    loss  = loss_function(output, target_labels)\n",
    "    test_loss += loss.item()\n",
    "    \n",
    "    #print(\"output\", torch.argmax(output, dim=1))\n",
    "    #print(\"gold\", target_labels.view(-1))\n",
    "    \n",
    "    correct_guesses += torch.sum(torch.eq(torch.argmax(output, dim=1), target_labels.view(-1)).long())\n",
    "\n",
    "accuracy = int(correct_guesses) / ((i+1) * batch_size)\n",
    "\n",
    "print('>', np.round(test_loss/(i+1), 4))\n",
    "print('BERT accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the difference between the first and second approach. What kind of representations are the different approaches using to predict word-senses? **[4 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In the first approach we predicted based on the word selected by the index and in the second approach we predicted based on the final hidden state. The final hidden states contain the memory of the weights of the different layers while the indexed method selects the the hidden state of the word in question.\n",
    "\n",
    "    The first approach seems to work better based on the lower test loss. We expected the second approach to work better because it contains more information but maybe the first approach is better after all because it is more specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model with per-word-form *accuracy* and comment on the results you get, how does the model perform in comparison to the baseline, and how do the models compare to each other? \n",
    "\n",
    "Expand on the evaluation by sorting the word-forms by the number of senses they have. Are word-forms with fewer senses easier to predict? Give a short explanation of the results you get based on the number of senses per word.\n",
    "\n",
    "**[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of baseline:  0.314266929651545\n"
     ]
    }
   ],
   "source": [
    "# baseline accuracy\n",
    "with open('test.csv') as test:\n",
    "    t = test.readlines()\n",
    "    accuracy = 0\n",
    "    for line in t:\n",
    "        word, sense = line.split('\\t')[1], line.split('\\t')[0]\n",
    "        if sense == baseline[word]:\n",
    "            accuracy += 1\n",
    "    print('accuracy of baseline: ', accuracy/len(t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the LSTMs perform in comparison to BERT? What's the difference between representations obtained by the LSTMs and BERT? **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      The following table shows the accuracy results of these models, here explained:\n",
    "\n",
    "- Baseline: a model which picks the most common word sense for each word.\n",
    "- LSTM1: a LSTM model which picks the word sense according to the position of the word.\n",
    "- LSTM2: a LSTM model which predicts the sense based on the final hidden state.\n",
    "\n",
    "\n",
    "      As an extra experiment to improve the accuracy of our Bert model, we attempted to fine-tune the scheduler by modifying the number of warmup steps. \n",
    "\n",
    "\n",
    "- BERT no_scheduler: BERT with no scheduler.\n",
    "- BERT scheduler1: BERT with a num_warmup_steps of 2% of the training steps.\n",
    "- BERT scheduler2: BERT with a num_warmup_steps of 5% of the training steps.\n",
    "- BERT scheduler2_oops: BERT with a num_warmup_steps of 5% of the training steps after an accidental keyboard interrupt in the third epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          | Baseline          | LSTM1              | LSTM2               | BERT no_scheduler    | BERT scheduler1   | BERT scheduler2    | BERT scheduler2_oops   |\n",
    "|----------|-------------------|--------------------|---------------------|----------------------|-------------------|--------------------|--------------------|\n",
    "| Accuracy | 0.3143 | 0.6889 | 0.4117 | 0.0105 | 0.4538 | 0.4727 | 0.5275 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Ranking\n",
    "        1. LSTM1 👑\n",
    "        2. BERT scheduler2_oops 🥈\n",
    "        3. BERT scheduler2 🥉\n",
    "        4. BERT scheduler1 ✌\n",
    "        5. Baseline 👍\n",
    "        6. BERT no_scheduler 👎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Due to an unfortunate mistake, we did a keyboard interrupt while in the third epoch in training the model with 5% of warmup steps. After rerunning the training cell, we observed that, at the first epoch, the loss started 1 point lower and the decrease was noticeably slower in the second and third epochs. \n",
    "\n",
    "    This unplanned model is the one that gives a higher accuracy (of about 52%) among all of the trained BERT models. This might be due to two reasons: \n",
    "    \n",
    "        1. First of all, the randomization of the data (different for BERT scheduler2 and BERT scheduler2_oops).\n",
    "        \n",
    "        2. Second, we believe that it is possible that the data ended up doing five epochs instead of three (2 before the keyboard interrupt and 3 after). This leads us to hypothesize that, despite having to start the training again, part of the information was retained and, therefore, the resulting model was slightly more accurate. This is only a supposition, given that we do not really know what happened when the cell was interrupted or what was saved. If that were true, this would lead us to assume that an increase in the number of epochs might yield even better results, but whether the resulting model would be as good with different testing sets and what the right number of epochs would be is still an open question.\n",
    "\n",
    "    Surprisingly, the model that yields the best results is the first LSTM, the one that picks the most common sense depending on the position of the word - we were expecting the BERT to work better than the others. Since this is the first time we fine-tune BERT, we believe that this result is due to the our lack of familiarity and intuition fine-tuning it.\n",
    "\n",
    "    We are especially concerned about the results for the BERT no_scheduler, which are dramatically low. This definitely proves how important it is to use a scheduler and fine-tune it correctly. \n",
    "    The baseline model, which is very basic, yield surprisingly good results, especially comparing it to the other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The first LSTM model represents the target word in the context based on the index. The second one, on the other hand, represents the whole context using the final hidden state to predicts the word sense. BERT is similar to the second LSTM, although it uses pretrained embeddings, which might give it a higher accuracy in correctly fine-tuned models, and attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Note: the different fine-tunes of BERT (BERT no_scheduler, BERT scheduler 1 and scheduler2) were done in different notebooks and computers to save time (and sanity) (check ../Dancing\\ Potatoes/assignment-04/Judit and ../Dancing\\ Potatoes/assignment-04/Patricia). This is the reason why the results are not shown in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could we do to improve our LSTM word sense disambiguation models and our BERT model? **[4 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (Partially answered in the previous question)\n",
    "    \n",
    "    When it comes to Neural Networks, the data that we use to train, develop and test the model plays a very important role in its performance. As a consequence, we believe that improving the quality and size of data is the most straightforward way to improve the models. For that, we suggest adding more words senses to the dataset. If the data was not revised by human annotators, we suggest that they might be useful to attest to the quality of the data.\n",
    "\n",
    "    In our second LSTM model approach, we concatenated the hidden states before using the classifier. We could try to concatenate the cell states as well to get a different and perhaps better result (commented out in the building of LSTM2). According to Hadiwinoto et al.(2019) another way to improve LSTM word sense disambiguation models would be \"using pre-trained contextualized word representation\" i.e., BERT.\n",
    "    \n",
    "    To improve BERT, we could try different forms of fine-tuning to see what works best. Since our knowledge of BERT is quite limited, we would have to do trial and error to see which approach works best. We tried different forms of fine-tuning, especially with the warmup steps in the scheduler (see answer above), but did not get optimal results. We suggest trying other configurations to get the better results we were hoping to see for BERT. We could try the approach suggested by Hadiwinoto et al.(2019), to use \"linear projection of the hidden vectors, coupled with gating to filter the values\".\n",
    "\n",
    "    Bibliography: \n",
    "    Hadiwinoto, C., Ng, H. T., & Gan, W. C. (2019). Improved word sense disambiguation using pre-trained contextualized word representations. arXiv preprint arXiv:1910.00194. https://www.aclweb.org/anthology/D19-1533.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings:\n",
    "\n",
    "[1] Kågebäck, M., & Salomonsson, H. (2016). Word Sense Disambiguation using a Bidirectional LSTM. arXiv preprint arXiv:1606.03568.\n",
    "\n",
    "[2] https://cl.lingfil.uu.se/~nivre/master/NLP-LexSem.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvSemantics",
   "language": "python",
   "name": "venvsemantics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
