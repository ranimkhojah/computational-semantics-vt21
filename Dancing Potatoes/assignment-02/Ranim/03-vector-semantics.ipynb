{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Semantics and Semantic Composition\n",
    "\n",
    "Nikolai Ilinykh (with modifications of notebooks by Katrin Erk)\n",
    "\n",
    "Here, we are going to look at  \n",
    "    (i) how a word can be represented through the counts of its neighboring words,  \n",
    "    (ii) how we can construct different semantic compositions for the phrase similarity task,  \n",
    "    (iii) how we can reduce the computation space of our meaning representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Choose your task dataset and your reference corpus\n",
    "\n",
    "The task dataset is basically the reason why we are computing word vectors. We are going to work with the dataset of phrase similarity, proposed in [1].  \n",
    "The idea is to build a model that is able to capture differences between different combinations of intransitive verbs and different nouns.  \n",
    "These phrases can be of high and low similarity.  \n",
    "In addition, we have human ratings of how similar/dissimilar phrases are.  \n",
    "\n",
    "The reference phrase is a combination of a noun and intransitive verb. Two landmark phrases (with high and low similarity) are paired with the reference phrase to produce a single item. Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](lapataexample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference phrase: the fire glowed (noun + reference)\n",
    "\n",
    "Landmark phrases:  \n",
    "High similarity phrase: the fire burned (noun + verb from high) (this phrase is HIGHLY similar to the reference phrase)  \n",
    "Low similarity phrase: the fire beamed (noun + verb from low) (this phrase is NOT REALLY similar to the reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to construct semantic space for our task dataset. The best solution would be to use the dataset used in [1], which is British National Corpus.  \n",
    "But for simplicity and purpose of this lecture, we are going to work with Gutenberg project, offered by nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our Gutenberg corpus and import necessary packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "gutenberg_files = glob.glob('./lecture7-code-vt2017/gutenberg/*.txt')\n",
    "gutenberg_files = gutenberg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./lecture7-code-vt2017/gutenberg/blake-poems.txt',\n",
       " './lecture7-code-vt2017/gutenberg/carroll-alice.txt',\n",
       " './lecture7-code-vt2017/gutenberg/whitman-leaves.txt',\n",
       " './lecture7-code-vt2017/gutenberg/milton-paradise.txt',\n",
       " './lecture7-code-vt2017/gutenberg/bible-kjv.txt',\n",
       " './lecture7-code-vt2017/gutenberg/austen-persuasion.txt',\n",
       " './lecture7-code-vt2017/gutenberg/melville-moby_dick.txt',\n",
       " './lecture7-code-vt2017/gutenberg/edgeworth-parents.txt',\n",
       " './lecture7-code-vt2017/gutenberg/chesterton-thursday.txt',\n",
       " './lecture7-code-vt2017/gutenberg/burgess-busterbrown.txt',\n",
       " './lecture7-code-vt2017/gutenberg/austen-emma.txt',\n",
       " './lecture7-code-vt2017/gutenberg/chesterton-brown.txt',\n",
       " './lecture7-code-vt2017/gutenberg/shakespeare-hamlet.txt',\n",
       " './lecture7-code-vt2017/gutenberg/austen-sense.txt',\n",
       " './lecture7-code-vt2017/gutenberg/shakespeare-macbeth.txt',\n",
       " './lecture7-code-vt2017/gutenberg/bryant-stories.txt']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['making', 'important', 'test']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(s):\n",
    "    '''\n",
    "    split text into words, lowercase them, remove punctuation and stopwords\n",
    "    we need to keep words which makes sense and can be informative for our task\n",
    "    '''\n",
    "    return [w.lower().strip(string.punctuation) for w in s.split() if w.lower() not in stopwords.words('english')]\n",
    "\n",
    "# testing our function\n",
    "preprocess(\"I am making a very important test.!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_word_count(corpus):\n",
    "    '''\n",
    "    let's count all words in our texts\n",
    "    we will need this information to ignore the least frequent words\n",
    "    why? least frequent words are typically not that informative\n",
    "    '''\n",
    "    word_count = nltk.FreqDist()\n",
    "    for filename in corpus:\n",
    "        print('reading file', filename)\n",
    "        with open(filename, 'r') as f1:\n",
    "            text = f1.read()\n",
    "            word_count.update(preprocess(text))\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file ./lecture7-code-vt2017/gutenberg/blake-poems.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/carroll-alice.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/whitman-leaves.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/milton-paradise.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/bible-kjv.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/austen-persuasion.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/melville-moby_dick.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/edgeworth-parents.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/chesterton-thursday.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/burgess-busterbrown.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/austen-emma.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/chesterton-brown.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/shakespeare-hamlet.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/austen-sense.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/shakespeare-macbeth.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/bryant-stories.txt\n"
     ]
    }
   ],
   "source": [
    "dataset = do_word_count(gutenberg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our dataset (good to keep files)\n",
    "#import pickle\n",
    "pickle.dump(dataset, open('./gutenberg_dataset.txt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_dataset = pickle.load(open('./gutenberg_dataset.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_corpus = our_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'shall': 11499, 'unto': 9010, 'said': 8738, 'lord': 8387, 'thou': 6622, 'one': 5724, 'him': 5713, 'thy': 5544, 'god': 5028, 'it': 4868, ...})"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our task dataset is different from reference corpus, we need to make sure that all words from the task dataset can be found in our reference corpus.  \n",
    "Otherwise, we will not be able to build vectors based on co-occurrences simply because our words of interest have never appeared in the reference texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant verb noun landmark input hilo\n",
      "participant20 stray thought roam 7 low\n",
      "participant20 stray discussion digress 6 high\n",
      "participant20 stray eye roam 7 high\n",
      "participant20 stray child digress 1 low\n",
      "participant20 throb body pulse 5 high\n",
      "participant20 throb head shudder 2 low\n",
      "participant20 throb voice shudder 3 low\n",
      "participant20 throb vein pulse 6 high\n",
      "participant20 chatter machine click 4 high\n"
     ]
    }
   ],
   "source": [
    "# load the task dataset\n",
    "with open('./mitchell_lapata_acl08.txt', 'r') as f:\n",
    "    phrase_dataset = f.read().splitlines()\n",
    "\n",
    "for line in phrase_dataset[:10]:\n",
    "    print(line)\n",
    "    \n",
    "# get all unique words\n",
    "words = []\n",
    "for line in phrase_dataset[1:]:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb not in words:\n",
    "        words.append(verb)\n",
    "    if noun not in words:\n",
    "        words.append(noun)\n",
    "    if landmark not in words:\n",
    "        words.append(landmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "participant20 throb body pulse 5 high\n",
    "\n",
    "participant id = participant 20\n",
    "the task for the human: evaluate landmark phrase and reference phrase and say on the scale from 1 to 7, how similar they are? 1 is not similar, 7 is very similar\n",
    "\n",
    "reference phrase: body throb\n",
    "landmark phrase: budy pulse (this phrase is HIGH in similarity to the reference, and we expect human to rate it accordingly)\n",
    "\n",
    "now, human gives us rating of 5; kind of high\n",
    "\n",
    "\n",
    "input: HUMAN judgements (1 - 7)\n",
    "hilo (high/low): some known truth about similarity between these phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ricochet\n",
      "flick\n",
      "slump\n",
      "erupt\n",
      "export\n",
      "fluctuate\n"
     ]
    }
   ],
   "source": [
    "# simply check if all words that we have in our task dataset can be found in the reference corpus (the result should return nothing)\n",
    "to_remove = []\n",
    "for w in words:\n",
    "    if w not in our_dataset:\n",
    "        print(w)\n",
    "        to_remove.append(w)\n",
    "# if something is not found, makes sense to ignore phrases with such non-present words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "# how many words do we have before cleaning?\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the task dataset (we might call it phrase dataset from now on)\n",
    "# we are removing all phrases which contain non-found words\n",
    "# this would probably remove other words as well (those, which are paired with the non-found words)\n",
    "\n",
    "cleaned_phrase_dataset = []\n",
    "for line in phrase_dataset:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb in to_remove or noun in to_remove or landmark in to_remove:\n",
    "        continue\n",
    "    cleaned_phrase_dataset.append(line)\n",
    "\n",
    "target_words = []\n",
    "for line in cleaned_phrase_dataset[1:]:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb not in target_words:\n",
    "        target_words.append(verb)\n",
    "    if noun not in target_words:\n",
    "        target_words.append(noun)\n",
    "    if landmark not in target_words:\n",
    "        target_words.append(landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words do we have after cleaning?\n",
    "len(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now our task dataset (phrase similarity dataset) matches our semantic space: all words in the task dataset OCCUR in our semantic space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file ./lecture7-code-vt2017/gutenberg/blake-poems.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/carroll-alice.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/whitman-leaves.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/milton-paradise.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/bible-kjv.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/austen-persuasion.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/melville-moby_dick.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/edgeworth-parents.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/chesterton-thursday.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/burgess-busterbrown.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/austen-emma.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/chesterton-brown.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/shakespeare-hamlet.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/austen-sense.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/shakespeare-macbeth.txt\n",
      "reading file ./lecture7-code-vt2017/gutenberg/bryant-stories.txt\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for filename in gutenberg_files:\n",
    "    print('reading file', filename)\n",
    "    with open(filename, 'r') as f1:\n",
    "        for line in f1:  \n",
    "            words = [w.lower().strip(string.punctuation) for w in line.split()]\n",
    "            if words != []:\n",
    "                for elem in words:\n",
    "                    all_words.append(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the semantic space for our target words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_space(window_size, corpus):\n",
    "    '''\n",
    "    this function builds semantic space for each word in the corpus\n",
    "    the space is limited by the window_size : how many words on both sides of the target word should be used\n",
    "    the space is counting frequency of context words, e.g. the idea of co-occurence\n",
    "    '''\n",
    "    \n",
    "    space = nltk.ConditionalFreqDist()\n",
    "    #\n",
    "    for index in tqdm.tqdm(range(len(corpus))):\n",
    "        # current word\n",
    "        current = corpus[index]\n",
    "        if current in target_words:\n",
    "        \n",
    "            # get future context for the first word only\n",
    "            # because there is no past context for the first word\n",
    "            if index == 0:\n",
    "                for cxword_index_after in range(window_size):\n",
    "                    cxword_after = corpus[cxword_index_after + 1]\n",
    "                    space[current].update([cxword_after])      \n",
    "            # context before and after the current word: specified by context size\n",
    "            if index > 0:\n",
    "                # extract words within the context window for the current word that occur BEFORE\n",
    "                for cxword_index_before in range(max(index - window_size, 0), index):\n",
    "                    # range is inclsuive the first value but exclusive the second\n",
    "                    cxword_before = corpus[cxword_index_before]\n",
    "                    # In a ConditionalFreqDist, if 'current' is not a condition yet,\n",
    "                    # then accessing it creates a new empty FreqDist for 'current'\n",
    "                    # The FreqDist method inc() increments the count for the given item by one.\n",
    "                    space[current].update([cxword_before])\n",
    "\n",
    "                # extract words within the context window for the current word that occur AFTER\n",
    "                if index + window_size < len(corpus):\n",
    "                    for cxword_index_after in range(index, max(index + window_size, 0)):\n",
    "                        cxword_after = corpus[cxword_index_after + 1]\n",
    "                        space[current].update([cxword_after])\n",
    "\n",
    "                # if window_size AFTER exceeds the length of the corpus (needed for the words in the end)\n",
    "                else:\n",
    "                   for cxword_index_after in range(index + 1, len(corpus)):\n",
    "                        cxword_after = corpus[cxword_index_after]\n",
    "                        space[current].update([cxword_after])\n",
    "\n",
    "    return space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2033185/2033185 [00:02<00:00, 708980.67it/s]\n"
     ]
    }
   ],
   "source": [
    "sp = compute_space(5, all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 80 conditions>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stray',\n",
       " 'thought',\n",
       " 'roam',\n",
       " 'discussion',\n",
       " 'digress',\n",
       " 'eye',\n",
       " 'child',\n",
       " 'throb',\n",
       " 'body',\n",
       " 'pulse']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stray:\n",
      " [('the', 8), ('from', 5), ('to', 5), ('me', 3), ('and', 3), ('of', 3), ('or', 3), ('a', 3), ('might', 2), ('out', 2)] \n",
      "\n",
      "thought:\n",
      " [('i', 498), ('the', 481), ('of', 421), ('and', 339), ('he', 317), ('to', 316), ('it', 276), ('a', 246), ('that', 223), ('she', 209)] \n",
      "\n",
      "discussion:\n",
      " [('of', 12), ('and', 9), ('the', 8), ('to', 5), ('in', 4), ('a', 4), ('one', 4), ('it', 3), ('their', 3), ('an', 3)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('stray:\\n', sp['stray'].most_common(10), '\\n')\n",
    "print('thought:\\n', sp['thought'].most_common(10), '\\n')\n",
    "print('discussion:\\n', sp['discussion'].most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem: a lot of context words do not carry much sense in them  \n",
    "What happens if we increase the window_size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stray:\n",
      " [('the', 8), ('from', 5), ('to', 5), ('me', 3), ('and', 3), ('of', 3), ('or', 3), ('a', 3), ('might', 2), ('out', 2), ('with', 2), ('you', 2), ('side', 2), ('into', 2), ('near', 2), ('wish', 1), ('church', 1), ('then', 1), ('parson', 1), ('preach', 1), ('them', 1), ('that', 1), ('would', 1), ('pedler', 1), ('sweats', 1), ('his', 1), ('yet', 1), ('who', 1), ('can', 1), ('i', 1), ('follow', 1), ('through', 1), ('groves', 1), ('coral', 1), ('sporting', 1), ('quick', 1), ('glance', 1), ('thy', 1), ('henceforth', 1), (\"where'er\", 1), ('our', 1), (\"day's\", 1), ('work', 1), ('lies', 1), ('purpose', 1), ('popping', 1), ('off', 1), ('narwhales', 1), ('vagrant', 1), ('sea', 1), ('unicorns', 1), ('whenever', 1), ('oar', 1), ('bit', 1), ('plank', 1), ('fast', 1), ('lest', 1), ('guinea-hen', 1), ('fall', 1), ('again', 1), ('only', 1), ('four', 1), ('finally', 1), ('last', 1), ('merry-maker', 1), ('ran', 1), ('house', 1), ('not', 1), ('understand', 1), ('let', 1), ('little', 1), ('too', 1), ('returning', 1), ('exercise', 1), ('letter-boy', 1), ('on', 1), ('an', 1), ('obstinate', 1), ('mule', 1), ('as', 1), ('he', 1), ('took', 1), ('up', 1), ('any', 1), ('letter', 1), ('him', 1), ('how', 1), ('beautifully', 1)] \n",
      "\n",
      "thought:\n",
      " [('i', 498), ('the', 481), ('of', 421), ('and', 339), ('he', 317), ('to', 316), ('it', 276), ('a', 246), ('that', 223), ('she', 209), ('was', 204), ('you', 163), ('in', 147), ('have', 140), ('had', 130), ('but', 125), ('as', 120), ('be', 119), ('would', 110), ('for', 105), ('her', 101), ('so', 92), ('at', 87), ('his', 87), ('him', 84), ('not', 80), ('with', 79), ('this', 68), ('is', 66), ('all', 64), ('they', 62), ('what', 60), ('could', 59), ('me', 58), ('very', 55), ('my', 53), ('never', 51), ('no', 50), ('which', 49), ('should', 48), ('if', 47), ('alice', 44), ('when', 44), ('one', 44), ('were', 41), ('them', 41), ('by', 41), ('we', 40), ('must', 39), ('said', 39), ('on', 38), ('might', 37), ('or', 35), ('from', 35), ('now', 34), ('little', 34), ('than', 33), ('some', 32), ('well', 31), ('first', 31), ('been', 31), ('then', 30), ('only', 30), ('herself', 29), ('good', 29), ('any', 28), ('more', 28), ('there', 28), ('an', 28), ('mr', 28), ('how', 27), ('thing', 26), ('do', 25), ('their', 24), ('much', 23), ('every', 23), ('who', 23), ('old', 23), ('emma', 23), ('time', 22), ('such', 22), ('about', 22), ('come', 22), ('out', 21), ('your', 21), ('shall', 20), ('too', 20), ('yet', 20), ('man', 20), ('himself', 20), ('ever', 19), ('mrs', 19), ('perhaps', 18), ('most', 18), ('because', 18), ('while', 17), ('say', 17), ('always', 17), ('almost', 17), ('like', 16)] \n",
      "\n",
      "discussion:\n",
      " [('of', 12), ('and', 9), ('the', 8), ('to', 5), ('in', 4), ('a', 4), ('one', 4), ('it', 3), ('their', 3), ('an', 3), ('his', 2), ('her', 2), ('was', 2), ('only', 2), ('they', 2), ('were', 2), ('me', 2), ('as', 2), ('such', 2), ('that', 2), ('printed', 1), (\"preach'd\", 1), ('discussed', 1), ('eludes', 1), ('print', 1), ('is', 1), ('not', 1), ('sufficiently', 1), ('feel', 1), ('continual', 1), ('crofts', 1), (\"evening's\", 1), ('indulgence', 1), ('subjects', 1), ('which', 1), ('usual', 1), ('even', 1), ('lady', 1), ('russell', 1), ('merits', 1), ('anne', 1), (\"night's\", 1), ('detective', 1), ('who', 1), ('deep', 1), ('actual', 1), ('immediate', 1), ('cut', 1), ('short', 1), ('cultured', 1), ('said', 1), ('colonel', 1), ('but', 1), ('can', 1), ('made', 1), ('matter', 1), ('much', 1), ('among', 1), ('you', 1), ('pray', 1), ('excuse', 1), ('nothing', 1), ('so', 1), ('interesting', 1), ('concerns', 1), ('every', 1), ('letters--and', 1), ('friendship', 1), ('long', 1), ('under', 1), ('them', 1), ('succeeded', 1), ('open', 1), ('frequent', 1), ('hopes', 1), ('chances', 1), ('she', 1), ('conviction', 1), ('all', 1), ('farther', 1), ('confidential', 1), ('topic', 1), ('had', 1), ('better', 1), ('shyness', 1), ('nor', 1), ('reserve', 1), ('speedily', 1), ('discovered', 1), ('earned', 1), ('privilege', 1), ('intimate', 1), (\"sister's\", 1), ('disappointment', 1), ('by', 1), ('preparing', 1), ('marianne', 1), ('for', 1), ('its', 1), ('no', 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('stray:\\n', sp['stray'].most_common(100), '\\n')\n",
    "print('thought:\\n', sp['thought'].most_common(100), '\\n')\n",
    "print('discussion:\\n', sp['discussion'].most_common(100), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove stop words since they are not adding anything useful to our representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 52501/1002514 [00:00<00:01, 524977.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluding stopwords from the semantic space...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1002514/1002514 [00:02<00:00, 463026.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "\n",
    "filtered_words = [word for word in all_words if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "print('excluding stopwords from the semantic space...')\n",
    "\n",
    "sp2 = compute_space(5, filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(sp2, open('./vector-space-sp2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2 = pickle.load(open('vector-space-sp2.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stray:\n",
      " [('side', 3), ('might', 2), ('sun', 2), ('hands', 2), ('little', 2), ('near', 2), ('livelong', 1), ('day', 1), ('ever', 1), ('wish', 1), ('church', 1), ('parson', 1), ('preach', 1), ('drink', 1), ('sing', 1), ('drover', 1), ('watching', 1), ('drove', 1), ('sings', 1), ('would', 1), ('pedler', 1), ('sweats', 1), ('pack', 1), ('back', 1), ('purchaser', 1), ('keep', 1), ('teach', 1), ('straying', 1), ('yet', 1), ('follow', 1), ('whoever', 1), ('present', 1), ('hour', 1), ('words', 1), ('graze', 1), ('sea-weed', 1), ('pasture', 1), ('groves', 1), ('coral', 1), ('sporting', 1), ('quick', 1), ('glance', 1), ('show', 1), ('forth', 1), ('never', 1), ('thy', 1), ('henceforth', 1), (\"where'er\", 1), (\"day's\", 1), ('work', 1)] \n",
      "\n",
      "thought:\n",
      " [('would', 176), ('could', 116), ('said', 106), ('one', 91), ('never', 86), ('little', 77), ('must', 63), ('thought', 60), ('much', 59), ('time', 57), ('well', 57), ('good', 57), ('mr', 57), ('man', 55), ('might', 53), ('alice', 51), ('shall', 50), ('like', 48), ('thing', 47), ('first', 45), ('say', 44), ('know', 41), ('mrs', 40), ('see', 39), ('every', 39), ('come', 38), ('old', 36), ('upon', 35), ('great', 34), ('looked', 34), ('think', 32), ('way', 32), ('ever', 32), ('sure', 32), ('yet', 32), ('go', 31), ('make', 31), ('long', 31), ('always', 31), ('unto', 31), ('father', 30), ('saw', 30), ('though', 30), ('miss', 30), ('nothing', 29), ('take', 28), ('made', 28), ('god', 28), ('last', 28), ('even', 27)] \n",
      "\n",
      "discussion:\n",
      " [('one', 4), ('book', 2), ('given', 2), ('least', 2), ('could', 2), ('would', 2), ('every', 2), ('must', 2), ('perpetually', 1), ('printed', 1), (\"preach'd\", 1), ('discussed', 1), ('eludes', 1), ('print', 1), ('put', 1), ('whoever', 1), ('harden', 1), ('nerves', 1), ('sufficiently', 1), ('feel', 1), ('continual', 1), ('crofts', 1), ('business', 1), ('evil', 1), ('assisted', 1), ('however', 1), ('persuasion', 1), (\"evening's\", 1), ('indulgence', 1), ('subjects', 1), ('usual', 1), ('companions', 1), ('probably', 1), ('concern', 1), ('meet', 1), ('even', 1), ('lady', 1), ('russell', 1), ('merits', 1), ('anne', 1), ('understand', 1), ('nearest', 1), ('police', 1), ('station', 1), (\"night's\", 1), ('detective', 1), ('know', 1), ('tone', 1), ('talk', 1), ('terrible', 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('stray:\\n', sp2['stray'].most_common(50), '\\n')\n",
    "print('thought:\\n', sp2['thought'].most_common(50), '\\n')\n",
    "print('discussion:\\n', sp2['discussion'].most_common(50), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about punctuation? We can also remove punctuation from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 105940/1001816 [00:00<00:01, 508795.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluding punctuation from the semantic space...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001816/1001816 [00:01<00:00, 544639.56it/s]\n"
     ]
    }
   ],
   "source": [
    "real_words = [w for w in filtered_words if w not in string.punctuation]\n",
    "print('excluding punctuation from the semantic space...')\n",
    "\n",
    "sp3 = compute_space(5, real_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stray:\n",
      " [('side', 3), ('might', 2), ('sun', 2), ('hands', 2), ('little', 2), ('near', 2), ('livelong', 1), ('day', 1), ('ever', 1), ('wish', 1), ('church', 1), ('parson', 1), ('preach', 1), ('drink', 1), ('sing', 1), ('drover', 1), ('watching', 1), ('drove', 1), ('sings', 1), ('would', 1), ('pedler', 1), ('sweats', 1), ('pack', 1), ('back', 1), ('purchaser', 1), ('keep', 1), ('teach', 1), ('straying', 1), ('yet', 1), ('follow', 1), ('whoever', 1), ('present', 1), ('hour', 1), ('words', 1), ('graze', 1), ('sea-weed', 1), ('pasture', 1), ('groves', 1), ('coral', 1), ('sporting', 1), ('quick', 1), ('glance', 1), ('show', 1), ('forth', 1), ('never', 1), ('thy', 1), ('henceforth', 1), (\"where'er\", 1), (\"day's\", 1), ('work', 1), ('lies', 1), ('though', 1), ('powder', 1), ('flask', 1), ('shot', 1), ('purpose', 1), ('popping', 1), ('narwhales', 1), ('vagrant', 1), ('sea', 1), ('unicorns', 1), ('infesting', 1), ('feeling', 1), ('flukes', 1), ('whenever', 1), ('oar', 1), ('bit', 1), ('plank', 1), ('least', 1), ('chip', 1), ('gate', 1), ('fast', 1), ('lest', 1), ('guinea-hen', 1), ('fall', 1), ('enemy', 1), ('miss', 1), ('barbara', 1), ('garden', 1), ('soon', 1), ('four', 1), ('finally', 1), ('last', 1), ('merry-maker', 1), ('ran', 1), ('house', 1), ('whooping', 1), ('companions', 1), ('professor', 1), ('head', 1), ('understand', 1), ('let', 1), ('hell', 1), ('gogol', 1), ('said', 1), ('mr', 1), (\"cole's\", 1), ('carriage-horses', 1), ('returning', 1), ('exercise', 1)] \n",
      "\n",
      "thought:\n",
      " [('would', 176), ('could', 116), ('said', 106), ('one', 91), ('never', 86), ('little', 77), ('must', 63), ('thought', 60), ('much', 59), ('time', 57), ('well', 57), ('good', 57), ('mr', 57), ('man', 55), ('might', 53), ('alice', 51), ('shall', 50), ('like', 48), ('thing', 47), ('first', 45), ('say', 44), ('know', 41), ('mrs', 40), ('see', 39), ('every', 39), ('come', 38), ('old', 36), ('upon', 35), ('great', 34), ('looked', 34), ('think', 32), ('way', 32), ('ever', 32), ('sure', 32), ('yet', 32), ('go', 31), ('make', 31), ('long', 31), ('always', 31), ('unto', 31), ('father', 30), ('saw', 30), ('though', 30), ('miss', 30), ('nothing', 29), ('take', 28), ('made', 28), ('god', 28), ('last', 28), ('even', 27), ('perhaps', 27), ('day', 27), ('give', 27), ('right', 26), ('quite', 26), ('knew', 26), ('love', 25), ('went', 25), ('heard', 25), ('us', 25), ('thee', 25), ('emma', 25), ('poor', 24), ('done', 24), ('thy', 24), ('cried', 24), ('indeed', 23), ('almost', 23), ('death', 22), ('least', 22), ('came', 22), ('better', 22), ('ye', 22), ('dear', 21), ('thou', 21), ('best', 20), ('another', 20), ('soon', 20), ('told', 20), ('things', 19), ('away', 19), ('seen', 19), ('something', 19), ('young', 19), ('possible', 18), ('two', 18), ('till', 18), ('look', 18), ('house', 18), ('may', 18), ('put', 18), ('mother', 18), ('anne', 18), ('whether', 18), ('life', 17), ('head', 17), ('face', 17), ('people', 17), ('eyes', 17), ('often', 17)] \n",
      "\n",
      "discussion:\n",
      " [('one', 4), ('book', 2), ('given', 2), ('least', 2), ('could', 2), ('would', 2), ('every', 2), ('must', 2), ('perpetually', 1), ('printed', 1), (\"preach'd\", 1), ('discussed', 1), ('eludes', 1), ('print', 1), ('put', 1), ('whoever', 1), ('harden', 1), ('nerves', 1), ('sufficiently', 1), ('feel', 1), ('continual', 1), ('crofts', 1), ('business', 1), ('evil', 1), ('assisted', 1), ('however', 1), ('persuasion', 1), (\"evening's\", 1), ('indulgence', 1), ('subjects', 1), ('usual', 1), ('companions', 1), ('probably', 1), ('concern', 1), ('meet', 1), ('even', 1), ('lady', 1), ('russell', 1), ('merits', 1), ('anne', 1), ('understand', 1), ('nearest', 1), ('police', 1), ('station', 1), (\"night's\", 1), ('detective', 1), ('know', 1), ('tone', 1), ('talk', 1), ('terrible', 1), ('purport', 1), ('deep', 1), ('actual', 1), ('immediate', 1), ('plot', 1), ('waiter', 1), ('downstairs', 1), ('secretary', 1), ('sorry', 1), ('cut', 1), ('short', 1), ('cultured', 1), ('said', 1), ('colonel', 1), ('lend', 1), ('motor-car', 1), ('two', 1), (\"smith's\", 1), ('intimacy', 1), ('made', 1), ('matter', 1), ('much', 1), ('among', 1), ('pray', 1), ('excuse', 1), ('supposing', 1), ('little', 1), ('mr', 1), ('elton', 1), ('found', 1), ('nothing', 1), ('interesting', 1), ('concerns', 1), ('report', 1), ('therefore', 1), ('post-office--catching', 1), ('cold--fetching', 1), ('letters--and', 1), ('friendship', 1), ('long', 1), ('succeeded', 1), ('equally', 1), ('thing', 1), ('like', 1), ('unreserve', 1), ('open', 1), ('frequent', 1), ('hopes', 1), ('chances', 1), ('perfectly', 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('stray:\\n', sp3['stray'].most_common(100), '\\n')\n",
    "print('thought:\\n', sp3['thought'].most_common(100), '\\n')\n",
    "print('discussion:\\n', sp3['discussion'].most_common(100), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our semantic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sp3, open('./vector-space.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discussion:\n",
      " [('one', 4), ('book', 2), ('given', 2), ('least', 2), ('could', 2), ('would', 2), ('every', 2), ('must', 2), ('perpetually', 1), ('printed', 1), (\"preach'd\", 1), ('discussed', 1), ('eludes', 1), ('print', 1), ('put', 1), ('whoever', 1), ('harden', 1), ('nerves', 1), ('sufficiently', 1), ('feel', 1), ('continual', 1), ('crofts', 1), ('business', 1), ('evil', 1), ('assisted', 1), ('however', 1), ('persuasion', 1), (\"evening's\", 1), ('indulgence', 1), ('subjects', 1), ('usual', 1), ('companions', 1), ('probably', 1), ('concern', 1), ('meet', 1), ('even', 1), ('lady', 1), ('russell', 1), ('merits', 1), ('anne', 1), ('understand', 1), ('nearest', 1), ('police', 1), ('station', 1), (\"night's\", 1), ('detective', 1), ('know', 1), ('tone', 1), ('talk', 1), ('terrible', 1), ('purport', 1), ('deep', 1), ('actual', 1), ('immediate', 1), ('plot', 1), ('waiter', 1), ('downstairs', 1), ('secretary', 1), ('sorry', 1), ('cut', 1), ('short', 1), ('cultured', 1), ('said', 1), ('colonel', 1), ('lend', 1), ('motor-car', 1), ('two', 1), (\"smith's\", 1), ('intimacy', 1), ('made', 1), ('matter', 1), ('much', 1), ('among', 1), ('pray', 1), ('excuse', 1), ('supposing', 1), ('little', 1), ('mr', 1), ('elton', 1), ('found', 1), ('nothing', 1), ('interesting', 1), ('concerns', 1), ('report', 1), ('therefore', 1), ('post-office--catching', 1), ('cold--fetching', 1), ('letters--and', 1), ('friendship', 1), ('long', 1), ('succeeded', 1), ('equally', 1), ('thing', 1), ('like', 1), ('unreserve', 1), ('open', 1), ('frequent', 1), ('hopes', 1), ('chances', 1), ('perfectly', 1)]\n"
     ]
    }
   ],
   "source": [
    "semantic_space = pickle.load(open('vector-space.p', 'rb'))\n",
    "\n",
    "print('discussion:\\n', semantic_space['discussion'].most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(semantic_space['discussion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to make sure that all words have the same number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {}\n",
    "for item in target_words:\n",
    "    # similar to [1],\n",
    "    # we identify the most frequent co-occurring words for all target words\n",
    "    # we need to take those, which seem to be appearing very often in different words' context\n",
    "    fixed_sem_space = dict(semantic_space[item])#.most_common(2000))\n",
    "    \n",
    "    for word, freq in fixed_sem_space.items():\n",
    "        if word not in columns:\n",
    "            columns[word] = freq\n",
    "        else:\n",
    "            if columns[word] < freq:\n",
    "                columns[word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_columns = sorted(columns.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every target word will be represented through these dimensions (most frequent and sensible words)\n",
    "dims = dict(sorted_columns[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 279.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# building our final semantic space for our target words\n",
    "target_semantic_space = {}\n",
    "for item in tqdm.tqdm(target_words):\n",
    "    target_semantic_space[item] = {}\n",
    "    this_sem_space = dict(semantic_space[item])\n",
    "    for other_w in dims.keys():\n",
    "        if other_w not in this_sem_space.keys():\n",
    "            target_semantic_space[item][other_w] = [0]\n",
    "        else:\n",
    "            target_semantic_space[item][other_w] = [this_sem_space[other_w]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shall  man  unto  said  every  god  lord  hand  thou  son  ...  edward  \\\n",
      "0    248   22   103    32     21   33   154    26    51   13  ...       1   \n",
      "\n",
      "   indignation  cease  deeds  half-chick  leaves  rage  prison  gifts  \\\n",
      "0            5      2      1           5       2     2       1      2   \n",
      "\n",
      "   accepted  \n",
      "0         1  \n",
      "\n",
      "[1 rows x 1103 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(target_semantic_space['fire'])\n",
    "m2 = (df != 0).all()\n",
    "print(df.loc[:, m2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shall  unto  said  every  god  lord  hand  thou  son  one  ...  breast  \\\n",
      "0     18     4     1      1    1     4     1     1    2    3  ...       2   \n",
      "\n",
      "   friendship  yield  hollow  generations  female  grown  expect  tower  \\\n",
      "0           1      1       1            1       1      1       1      2   \n",
      "\n",
      "   indignation  \n",
      "0            1  \n",
      "\n",
      "[1 rows x 343 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(target_semantic_space['flame'])\n",
    "m2 = (df != 0).all()\n",
    "print(df.loc[:, m2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shall  man  unto  said  every  hand  thou  one  thy  old  ...  2:11  \\\n",
      "0      3    4     1     2      2     1     8    1    2    2  ...     1   \n",
      "\n",
      "   plough  farewell  gath  cubits  glance  eternal  canst  pull  latter  \n",
      "0       3         1     2       1       1        1      2     1       1  \n",
      "\n",
      "[1 rows x 132 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(target_semantic_space['beam'])\n",
    "m2 = (df != 0).all()\n",
    "print(df.loc[:, m2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shall  man  unto  said  every  lord  hand  thou  one  old  ...  girded  \\\n",
      "0     39    7     2     7      2     2     2     6    3    2  ...       1   \n",
      "\n",
      "   canst  raiment  yield  hollow  fit  simply  cover  entire  giant  \n",
      "0      1        2      1       1    1       4      4       1      5  \n",
      "\n",
      "[1 rows x 331 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(target_semantic_space['skin'])\n",
    "m2 = (df != 0).all()\n",
    "print(df.loc[:, m2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shall  man  unto  said  every  god  lord  hand  thou  son  ...  edward  \\\n",
      "0    146   47    52   126     34   19    46    55    71   14  ...       3   \n",
      "\n",
      "   engaged  indignation  latter  half-chick  leaves  rage  prison  giant  \\\n",
      "0        1            1       1           1       5     2       5      2   \n",
      "\n",
      "   accepted  \n",
      "0         2  \n",
      "\n",
      "[1 rows x 1436 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(target_semantic_space['head'])\n",
    "m2 = (df != 0).all()\n",
    "print(df.loc[:, m2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./target_semantic_space.json', 'w') as f2:\n",
    "#    json.dump(target_semantic_space, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./target_semantic_space.json', 'r') as f3:\n",
    "    our_space = json.load(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the task\n",
    "\n",
    "We want to make a model that is able to see the differences between phrases / sentences, not solely individual words.  \n",
    "Then, we will need to test this model and check whether its performance correlates with human judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important steps:  \n",
    "1. Our semantic space needs to cover a lot of words; this is why it is important to train it on the large corpus, e.g. Gutenberg corpus.  \n",
    "2. We need the dataset of phrases that have human judgements of how similar/dissimilar these phrases are. It is needed to later see which of our models performs the closest to the way humans deal with the task.  \n",
    "3. Therefore, we need to decide how we are going to combine representations from single words into a single phrase representation. Since we have every word represented through counts of words from its context, we can do the following operations:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = [15, 16, 5, 6] (stands for house, window_size = N)\n",
    "v = [4, 5, 3, 4] (stands for burn)\n",
    "\n",
    "house + burn = house burn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](simple_additive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](simple_multiplicative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](combined.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha, beta, gamma = three variables, which control how much each constituent contributes to the result\n",
    "\n",
    "if alpha is 0.0, it means that no contribution will be given by u\n",
    "then higher the variable is, the more contribution the count has\n",
    "\n",
    "alpha = 0, beta = 0.95, gamma - 0.05\n",
    "->>>> for 'house burn', if we go with the last method we are saying that meaning of 'house' is not needed, while mostly the meaning of\n",
    "the verb is needed, and the meaning of noun multiplied with verb is also needed a bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test these representations and see how different they are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['participant50 chatter child gabble 6 high',\n",
       " 'participant50 chatter tooth click 2 high',\n",
       " 'participant50 reel head whirl 5 high',\n",
       " 'participant50 reel mind stagger 4 low',\n",
       " 'participant50 reel industry stagger 5 high',\n",
       " 'participant50 reel man whirl 3 low',\n",
       " 'participant50 glow fire beam 7 low',\n",
       " 'participant50 glow face burn 3 low',\n",
       " 'participant50 glow cigar burn 5 high',\n",
       " 'participant50 glow skin beam 7 high']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_phrase_dataset[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['participant', 'verb', 'noun', 'landmark', 'input', 'hilo']\n"
     ]
    }
   ],
   "source": [
    "# the first line is the column name line, we ignore it\n",
    "column_names = phrase_dataset[0].split()\n",
    "print(column_names)\n",
    "\n",
    "dataset = {}\n",
    "references = []\n",
    "\n",
    "for line in phrase_dataset[1:]:\n",
    "    participant_id, reference, noun, landmark, rating, hilo = line.split()\n",
    "        \n",
    "    reference_phrase = [noun, reference]\n",
    "\n",
    "    if reference_phrase not in references:\n",
    "        references.append(reference_phrase)\n",
    "\n",
    "    landmark_phrase = [noun, landmark]\n",
    "\n",
    "    if participant_id not in dataset:\n",
    "        dataset[participant_id] = []\n",
    "    else:\n",
    "        dataset[participant_id].append((reference_phrase, landmark_phrase, rating, hilo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for item in dataset:\n",
    "#    for judgement in dataset[item]:\n",
    "#        if judgement[0][0] == 'face':\n",
    "#            print(judgement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['discussion', 'stray'], ['discussion', 'digress'], '6', 'high'),\n",
       " (['eye', 'stray'], ['eye', 'roam'], '1', 'high'),\n",
       " (['child', 'stray'], ['child', 'digress'], '1', 'low'),\n",
       " (['head', 'reel'], ['head', 'stagger'], '4', 'low'),\n",
       " (['mind', 'reel'], ['mind', 'whirl'], '5', 'high'),\n",
       " (['industry', 'reel'], ['industry', 'whirl'], '2', 'low'),\n",
       " (['man', 'reel'], ['man', 'stagger'], '5', 'high'),\n",
       " (['cigarette', 'flare'], ['cigarette', 'erupt'], '1', 'low'),\n",
       " (['eye', 'flare'], ['eye', 'flame'], '2', 'high'),\n",
       " (['argument', 'flare'], ['argument', 'erupt'], '6', 'high')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['participant1'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mind', 'reel'],\n",
       " ['head', 'reel'],\n",
       " ['man', 'reel'],\n",
       " ['fire', 'glow'],\n",
       " ['face', 'glow'],\n",
       " ['cigar', 'glow'],\n",
       " ['skin', 'glow']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[-7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we are going to combine representations of words into a single phrase representations?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['face', 'glow']\n"
     ]
    }
   ],
   "source": [
    "example_phrase = references[-3]\n",
    "print(example_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_space = our_space[example_phrase[0]]\n",
    "verb_space = our_space[example_phrase[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = ['face', 'glow']\n",
    "landmark_high = ['face', 'beam']\n",
    "landmark_low = ['face', 'burn']\n",
    "\n",
    "#face glow vs face beam (reference vs high similarity landmark), we also have a human rating\n",
    "#face glow vs face burn (reference vs low similarity landmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we access similarity between two phrases?\n",
    "\n",
    "The main way in which distributional vectors are used is for estimating similarity between words. The central idea is that words that appear in similar contexts tend to be similar in meaning. So what we need to do to estimate word similarity from distributional vectors is to compute a similarity measure that determines how similar the vectors of two words are. There are many similarity measures, but the one that is used the most is cosine similarity. If we consider the vector of a word as an arrow from the origin, then two words should be similar if their vectors go in roughly the same direction. Cosine similarity measures this as the cosine of the angle between the two vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cosine(u, v) = \\frac{\\sum_i {u_i v_i}}{|u| |v|}$$\n",
    "\n",
    "$$|v| = \\sqrt{\\sum_i v_i^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# similarity measure: cosine\n",
    "#                           sum_i vec1_i * vec2_i\n",
    "# cosine(vec1, vec2) = ------------------------------\n",
    "#                        veclen(vec1) * veclen(vec2)\n",
    "# where\n",
    "#\n",
    "# veclen(vec) = squareroot( sum_i vec_i*vec_i )\n",
    "#\n",
    "\n",
    "def veclen(vector):\n",
    "    return math.sqrt(np.sum(np.square(vector)))\n",
    "\n",
    "def cosine(vector1, vector2):\n",
    "    veclen1 = veclen(vector1)\n",
    "    veclen2 = veclen(vector2)\n",
    "    if veclen1 == 0.0 or veclen2 == 0.0:\n",
    "        # one of the vectors is empty, the cosine is 0\n",
    "        return 0.0\n",
    "    else:\n",
    "        # we could also simply do:\n",
    "        dotproduct = np.dot(vector1, vector2)\n",
    "        return dotproduct / (veclen1 * veclen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phrase_space(phrase, x_names):\n",
    "\n",
    "    # first we get representations for verb and noun\n",
    "    subject_space = our_space[phrase[0]]\n",
    "    verb_space = our_space[phrase[1]]\n",
    "    # representation for house and burn\n",
    "    \n",
    "    representation = np.zeros(len(x_names))\n",
    "\n",
    "    for n, word in enumerate(x_names.keys()):\n",
    "\n",
    "        # I get v^Ith element from each of the vectors\n",
    "        subject_value = subject_space[word][0]\n",
    "        verb_value = verb_space[word][0]\n",
    "\n",
    "        #out = subject_value + verb_value\n",
    "        #out = subject_value * verb_value\n",
    "        \n",
    "        # 6 and 0, if we do summation, we are getting 6\n",
    "        # if we do mulitplication, we are getting 0\n",
    "        \n",
    "        #out = subject_value * 0.2 + verb_value * 0.8\n",
    "        #out = subject_value * 0.0 + verb_value * 0.95 + (0.05 * subject_value * verb_value)\n",
    "\n",
    "        representation[n] = out\n",
    "\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119.  85.   0. ...   0.   0.   0.] (2000,)\n",
      "[357. 340.  83. ...   0.   0.   0.]\n",
      "[11186.   340.  2490. ...     0.     0.     0.]\n"
     ]
    }
   ],
   "source": [
    "reference = ['face', 'glow']\n",
    "landmark_high = ['face', 'beam']\n",
    "landmark_low = ['face', 'burn']\n",
    "\n",
    "ref = build_phrase_space(reference, dims)\n",
    "\n",
    "lhigh = build_phrase_space(landmark_high, dims)\n",
    "\n",
    "llow = build_phrase_space(landmark_low, dims)\n",
    "\n",
    "\n",
    "print(ref, ref.shape)\n",
    "print(lhigh)\n",
    "print(llow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26726492389325335"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(ref, lhigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22604548518015988"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(ref, llow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test how the cosine similarity between vectors of each of our spaces compares with the human judgements on the words collected in the previous step. Which of the three spaces best approximates human judgements?\n",
    "\n",
    "For comparison of several scores we can use [Spearman correlation coefficient](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient) which is implemented in `scipy.stats.spearmanr` [here](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.stats.spearmanr.html). The values of the Sperman correlation coefficient range from -1, 0 to 1, where 0 indicates no correlation, 1 perfect correaltion and -1 negative correlation. Hence, the greater the number the better. The p values tells us if the coefficient is statistically significant. For this to be the case, it must be less than or equal to $< 0.05$.\n",
    "\n",
    "Some guidelines for implementing the correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = {}\n",
    "j['landmark_high'] = []\n",
    "j['landmark_low'] = []\n",
    "\n",
    "for participant in dataset:\n",
    "    #if participant == 'participant8':\n",
    "    judgements = dataset[participant]\n",
    "    for ref, landmark, score, gt in judgements:\n",
    "        if ['face', 'glow'] == ref and landmark == ['face', 'beam']:\n",
    "            j['landmark_high'].append(score)\n",
    "        if ['face', 'glow'] == ref and landmark == ['face', 'burn']:\n",
    "            j['landmark_low'].append(score)\n",
    "\n",
    "#dataset['participant1'][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(j['landmark_high']))\n",
    "print(len(j['landmark_low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 6, 2, 6, 4, 5, 7, 3, 4, 6, 5, 5, 7, 6, 6, 7, 6, 6, 2, 5, 5,\n",
       "       6, 7, 6, 4, 6, 6, 7, 4, 5, 6, 5, 5])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high = np.array([int(elem) for elem in j['landmark_high']])\n",
    "high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 3, 3, 4, 4, 5, 2, 3, 1, 1, 7, 2, 5, 4, 1, 4, 1, 6, 7, 4, 7,\n",
       "       3, 4, 7, 3])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low = np.array([int(elem) for elem in j['landmark_low']])\n",
    "low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: try to correlate model's predictions with each human individually. For example, for model A (simple additive), you would have two conditions: high and low rankings. First, gather human judgements for N-high-rated pairs. Let's say, you have one judgement from a single human for N=5 pairs. For the same pairs, gather model A cosine scores. In the end, tou should have two vectors of same size. Now, compute Spearman correlation on these two vectors and the result will be the correlation between model's predictions and human judgements for highly rated phrases. Similar procedure should be done for pairs with low ratings.  \n",
    "\n",
    "Question: what would you do to get one single number for *all* human judgements vs. each model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming counts to association weights\n",
    "\n",
    "All words will have high co-occurrence counts with the most frequent context items. In our demo dataset, these are i-PR, the-DT, man-NN, on-CD, could-MD. This will falsely inflate all our similarity estimates. What we want to know instead is how strongly a target word is associated with a context item: Does it appear with the context item more often than we could expect at random? Less often? About as often as we would expect? \n",
    "\n",
    "There are multiple options for computing degree of association:\n",
    "\n",
    "- tf-idf (term frequency / inverse document frequency)\n",
    "- pointwise mutual information (PMI)\n",
    "- positive mutual information (PPMI): just change negative PMI values to zero\n",
    "- local mutual information (LMI)\n",
    "\n",
    "We do PPMI here. The PMI of a target word t and context item c is defined as:\n",
    " \n",
    "$$PMI(t, c) = log \\frac{P(t, c)}{P(t) P(c)}$$\n",
    "\n",
    "All the probabilities are computed from the table of counts. We need:\n",
    "\n",
    "- $\\#(t, c)$: the co-occurrence count of t with c\n",
    "- $\\#(\\_, \\_)$: the sum of counts in the whole table, across all targets\n",
    "- $\\#(t, \\_)$: the sum of counts in the row of target t\n",
    "- $\\#(\\_, c)$: the sum of counts in the column of context item c\n",
    "\n",
    "Then we have: \n",
    "\n",
    "- $P(t, c) = \\frac{\\#(t, c)}{\\#(\\_,\\_)}$\n",
    "- $P(t) = \\frac{\\#(t,\\_)}{\\#(\\_,\\_)}$\n",
    "- $P(c) = \\frac{\\#(\\_,c)}{\\#(\\_,\\_)}$\n",
    "\n",
    "Here is the code for computing PPMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# transform the space using positive pointwise mutual information\n",
    "\n",
    "# target t, dimension value c, then\n",
    "# PMI(t, c) = log ( P(t, c) / (P(t) P(c)) )\n",
    "# where\n",
    "# P(t, c) = #(t, c) / #(_, _)\n",
    "# P(t) = #(t, _) / #(_, _)\n",
    "# P(c) = #(_, c) / #(_, _)\n",
    "#\n",
    "# PPMI(t, c) =   PMI(t, c) if PMI(t, c) > 0\n",
    "#                0 else\n",
    "\n",
    "def ppmi_transform(space, word):\n",
    "    \n",
    "    row_sums = {}\n",
    "    col_sums = {}\n",
    "    context_word = {}\n",
    "    \n",
    "    pmi_return = {}\n",
    "    \n",
    "    #(_, _): overall count of occurrences\n",
    "    overall = 0\n",
    "    for _, vectors in space.items():\n",
    "        for _, f in vectors.items():\n",
    "            overall += f[0]    \n",
    "    \n",
    "    for t, vectors in space.items():\n",
    "        \n",
    "        if t == word:\n",
    "            # #(t, _): for each target word, sum up all its counts.\n",
    "            # row_sums is a dictionary mapping from target words to row sums\n",
    "            # how many time t appears in the context\n",
    "            t_sum = 0\n",
    "            for _, f in vectors.items():\n",
    "                t_sum += f[0]\n",
    "            row_sums[t] = t_sum\n",
    "            \n",
    "        # #(_, c): for each context word, sum up all its counts\n",
    "        # col_sums is a dictionary mapping from context word indices to column sums\n",
    "        #col_sums = {}\n",
    "        #for c, f in vectors.items():\n",
    "        #    col_sums[c] = sum([elem[0] for elem in f])\n",
    "        #print(col_sums)\n",
    "        \n",
    "        # #(_, c): for each context word, sum up all its counts\n",
    "        # col_sums is a dictionary mapping from context word indices to column sums\n",
    "        for c, f in vectors.items():\n",
    "            if c not in col_sums:\n",
    "                col_sums[c] = f[0]\n",
    "            else:\n",
    "                col_sums[c] += f[0]\n",
    "\n",
    "    #print(col_sums)\n",
    "    #print(row_sums)\n",
    "    #print(context_word)\n",
    "    pmi_return[word] = {}\n",
    "    \n",
    "    for context_word, context_sums in col_sums.items():\n",
    "        target_pmi = np.log2((context_sums / overall) / (row_sums[word] / overall) * (col_sums[context_word] / overall))\n",
    "\n",
    "        pmi_return[word][context_word] = target_pmi\n",
    "        \n",
    "    return pmi_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stray\n",
      "-0.35357644493511 shall\n",
      "-2.2496496016922087 man\n",
      "-1.8865614322739177 unto\n",
      "-1.2478531298268456 said\n",
      "-3.1559022722260726 every\n",
      "thought\n",
      "-7.2851463147162 shall\n",
      "-9.181219471473298 man\n",
      "-8.818131302055008 unto\n",
      "-8.179422999607935 said\n",
      "-10.087472142007162 every\n",
      "roam\n",
      "1.0412831724061036 shall\n",
      "-0.8547899843509952 man\n",
      "-0.491701814932704 unto\n",
      "0.14700648751436796 said\n",
      "-1.7610426548848588 every\n",
      "discussion\n",
      "-0.5613813300485115 shall\n",
      "-2.4574544868056107 man\n",
      "-2.0943663173873195 unto\n",
      "-1.4556580149402472 said\n",
      "-3.363707157339474 every\n",
      "digress\n",
      "2.988815752511968 shall\n",
      "1.0927425957548689 man\n",
      "1.4558307651731601 unto\n",
      "2.094539067620232 said\n",
      "0.1864899252210054 every\n",
      "eye\n",
      "-5.65091255164008 shall\n",
      "-7.546985708397179 man\n",
      "-7.183897538978888 unto\n",
      "-6.545189236531816 said\n",
      "-8.453238378931042 every\n",
      "child\n",
      "-6.046572506162086 shall\n",
      "-7.942645662919184 man\n",
      "-7.579557493500893 unto\n",
      "-6.940849191053822 said\n",
      "-8.848898333453048 every\n",
      "throb\n",
      "0.6668876576246057 shall\n",
      "-1.2291854991324933 man\n",
      "-0.8660973297142021 unto\n",
      "-0.2273890272671301 said\n",
      "-2.1354381696663567 every\n",
      "body\n",
      "-6.369992408076541 shall\n",
      "-8.26606556483364 man\n",
      "-7.902977395415349 unto\n",
      "-7.264269092968277 said\n",
      "-9.172318235367504 every\n",
      "pulse\n",
      "-1.52575742031779 shall\n",
      "-3.421830577074889 man\n",
      "-3.058742407656598 unto\n",
      "-2.4200341052095258 said\n",
      "-4.3280832476087525 every\n",
      "head\n",
      "-7.227063882276415 shall\n",
      "-9.123137039033514 man\n",
      "-8.760048869615222 unto\n",
      "-8.12134056716815 said\n",
      "-10.029389709567377 every\n",
      "shudder\n",
      "-0.13456666299331418 shall\n",
      "-2.030639819750413 man\n",
      "-1.667551650332122 unto\n",
      "-1.02884334788505 said\n",
      "-2.9368924902842766 every\n",
      "voice\n",
      "-7.20400542491687 shall\n",
      "-9.10007858167397 man\n",
      "-8.736990412255679 unto\n",
      "-8.098282109808606 said\n",
      "-10.006331252207833 every\n",
      "vein\n",
      "1.0957309564284798 shall\n",
      "-0.800342200328619 man\n",
      "-0.4372540309103279 unto\n",
      "0.20145427153674422 said\n",
      "-1.7065948708624825 every\n",
      "chatter\n",
      "1.2726087185125592 shall\n",
      "-0.6234644382445398 man\n",
      "-0.2603762688262484 unto\n",
      "0.3783320336208234 said\n",
      "-1.5297171087784032 every\n",
      "machine\n",
      "-1.5701515396762438 shall\n",
      "-3.4662246964333425 man\n",
      "-3.1031365270150513 unto\n",
      "-2.4644282245679796 said\n",
      "-4.372477366967206 every\n",
      "click\n",
      "1.9888157525119678 shall\n",
      "0.09274259575486894 man\n",
      "0.4558307651731601 unto\n",
      "1.0945390676202318 said\n",
      "-0.8135100747789946 every\n",
      "girl\n",
      "-4.883309424935874 shall\n",
      "-6.779382581692973 man\n",
      "-6.416294412274682 unto\n",
      "-5.77758610982761 said\n",
      "-7.685635252226836 every\n",
      "gabble\n",
      "2.336739055932275 shall\n",
      "0.44066589917517573 man\n",
      "0.8037540685934673 unto\n",
      "1.4424623710405393 said\n",
      "-0.46558677135868753 every\n",
      "tooth\n",
      "-1.6047087617126121 shall\n",
      "-3.500781918469711 man\n",
      "-3.13769374905142 unto\n",
      "-2.498985446604348 said\n",
      "-4.407034589003575 every\n",
      "rebound\n",
      "2.988815752511968 shall\n",
      "1.0927425957548689 man\n",
      "1.4558307651731601 unto\n",
      "2.094539067620232 said\n",
      "0.1864899252210054 every\n",
      "ball\n",
      "-3.011184247488032 shall\n",
      "-4.907257404245131 man\n",
      "-4.54416923482684 unto\n",
      "-3.905460932379768 said\n",
      "-5.8135100747789945 every\n",
      "rally\n",
      "1.2726087185125592 shall\n",
      "-0.6234644382445398 man\n",
      "-0.2603762688262484 unto\n",
      "0.3783320336208234 said\n",
      "-1.5297171087784032 every\n",
      "optimism\n",
      "2.09573095642848 shall\n",
      "0.19965779967138103 man\n",
      "0.5627459690896721 unto\n",
      "1.2014542715367442 said\n",
      "-0.7065948708624824 every\n",
      "flicker\n",
      "1.4038532517908118 shall\n",
      "-0.4922199049662871 man\n",
      "-0.12913173554799592 unto\n",
      "0.509576566899076 said\n",
      "-1.3984725755001508 every\n",
      "tongue\n",
      "-4.736185250719275 shall\n",
      "-6.632258407476375 man\n",
      "-6.269170238058083 unto\n",
      "-5.630461935611011 said\n",
      "-7.538511078010238 every\n",
      "waver\n",
      "2.336739055932275 shall\n",
      "0.44066589917517573 man\n",
      "0.8037540685934673 unto\n",
      "1.4424623710405393 said\n",
      "-0.46558677135868753 every\n",
      "interest\n",
      "-4.45531173605064 shall\n",
      "-6.351384892807739 man\n",
      "-5.988296723389448 unto\n",
      "-5.349588420942376 said\n",
      "-7.257637563341603 every\n",
      "subside\n",
      "1.211208173848416 shall\n",
      "-0.684864982908683 man\n",
      "-0.3217768134903918 unto\n",
      "0.3169314889566801 said\n",
      "-1.5911176534425464 every\n",
      "flood\n",
      "-3.183968903069585 shall\n",
      "-5.080042059826684 man\n",
      "-4.7169538904083925 unto\n",
      "-4.07824558796132 said\n",
      "-5.986294730360547 every\n",
      "lessen\n",
      "-0.90426904357152 shall\n",
      "-2.800342200328619 man\n",
      "-2.437254030910328 unto\n",
      "-1.7985457284632558 said\n",
      "-3.7065948708624825 every\n",
      "fear\n",
      "-6.4314452688446515 shall\n",
      "-8.32751842560175 man\n",
      "-7.9644302561834595 unto\n",
      "-7.325721953736387 said\n",
      "-9.233771096135614 every\n",
      "symptom\n",
      "-0.11071992103894654 shall\n",
      "-2.006793077796045 man\n",
      "-1.6437049083777542 unto\n",
      "-1.004996605930682 said\n",
      "-2.9130457483299086 every\n",
      "sink\n",
      "-2.731306331490824 shall\n",
      "-4.627379488247923 man\n",
      "-4.264291318829632 unto\n",
      "-3.62558301638256 said\n",
      "-5.533632158781787 every\n",
      "island\n",
      "-2.4564947578806766 shall\n",
      "-4.352567914637776 man\n",
      "-3.9894797452194846 unto\n",
      "-3.3507714427724125 said\n",
      "-5.258820585171639 every\n",
      "bow\n",
      "-4.820378169209417 shall\n",
      "-6.716451325966516 man\n",
      "-6.353363156548225 unto\n",
      "-5.714654854101153 said\n",
      "-7.6227039965003796 every\n",
      "butler\n",
      "-2.447003308903379 shall\n",
      "-4.343076465660478 man\n",
      "-3.9799882962421864 unto\n",
      "-3.3412799937951148 said\n",
      "-5.2493291361943415 every\n",
      "submit\n",
      "-2.4517568388740134 shall\n",
      "-4.347829995631113 man\n",
      "-3.984741826212821 unto\n",
      "-3.3460335237657493 said\n",
      "-5.254082666164976 every\n",
      "company\n",
      "-5.19322382527516 shall\n",
      "-7.089296982032259 man\n",
      "-6.726208812613968 unto\n",
      "-6.087500510166896 said\n",
      "-7.9955496525661225 every\n",
      "stoop\n",
      "-1.0240082878456158 shall\n",
      "-2.920081444602715 man\n",
      "-2.5569932751844235 unto\n",
      "-1.9182849727373514 said\n",
      "-3.826334115136578 every\n",
      "government\n",
      "-1.2912921666807673 shall\n",
      "-3.1873653234378665 man\n",
      "-2.824277154019575 unto\n",
      "-2.185568851572503 said\n",
      "-4.09361799397173 every\n",
      "recoil\n",
      "1.4038532517908118 shall\n",
      "-0.4922199049662871 man\n",
      "-0.12913173554799592 unto\n",
      "0.509576566899076 said\n",
      "-1.3984725755001508 every\n",
      "heart\n",
      "-7.495000024752288 shall\n",
      "-9.391073181509388 man\n",
      "-9.027985012091095 unto\n",
      "-8.389276709644024 said\n",
      "-10.297325852043251 every\n",
      "flinch\n",
      "2.796170674569572 shall\n",
      "0.9000975178124732 man\n",
      "1.263185687230764 unto\n",
      "1.9018939896778364 said\n",
      "-0.006155152721390324 every\n",
      "rifle\n",
      "0.938189679442 shall\n",
      "-0.957883477315099 man\n",
      "-0.5947953078968079 unto\n",
      "0.04391299455026413 said\n",
      "-1.8641361478489624 every\n",
      "kick\n",
      "-1.4799537307046655 shall\n",
      "-3.3760268874617645 man\n",
      "-3.0129387180434732 unto\n",
      "-2.374230415596401 said\n",
      "-4.282279557995628 every\n",
      "hand\n",
      "-8.255633034473187 shall\n",
      "-10.151706191230284 man\n",
      "-9.788618021811994 unto\n",
      "-9.14990971936492 said\n",
      "-11.057958861764149 every\n",
      "boom\n",
      "1.4038532517908118 shall\n",
      "-0.4922199049662871 man\n",
      "-0.12913173554799592 unto\n",
      "0.509576566899076 said\n",
      "-1.3984725755001508 every\n",
      "noise\n",
      "-4.6916693592534795 shall\n",
      "-6.587742516010579 man\n",
      "-6.2246543465922874 unto\n",
      "-5.585946044145215 said\n",
      "-7.493995186544442 every\n",
      "prosper\n",
      "-2.9587168275938964 shall\n",
      "-4.854789984350995 man\n",
      "-4.491701814932704 unto\n",
      "-3.852993512485632 said\n",
      "-5.761042654884859 every\n",
      "sale\n",
      "-0.7887918261515839 shall\n",
      "-2.684864982908683 man\n",
      "-2.3217768134903918 unto\n",
      "-1.68306851104332 said\n",
      "-3.5911176534425464 every\n",
      "thunder\n",
      "-3.4205751836257337 shall\n",
      "-5.316648340382833 man\n",
      "-4.953560170964542 unto\n",
      "-4.3148518685174695 said\n",
      "-6.222901010916696 every\n",
      "gun\n",
      "-1.8404539459740767 shall\n",
      "-3.7365271027311757 man\n",
      "-3.373438933312884 unto\n",
      "-2.7347306308658124 said\n",
      "-4.6427797732650395 every\n",
      "concentration\n",
      "0.7517765552111186 shall\n",
      "-1.1442966015459803 man\n",
      "-0.7812084321276893 unto\n",
      "-0.14250012968061715 said\n",
      "-2.0505492720798437 every\n",
      "falter\n",
      "1.7087078333192327 shall\n",
      "-0.1873653234378663 man\n",
      "0.17572284598042512 unto\n",
      "0.8144311484274969 said\n",
      "-1.0936179939717297 every\n",
      "determination\n",
      "-1.1226925627050224 shall\n",
      "-3.0187657194621216 man\n",
      "-2.6556775500438303 unto\n",
      "-2.016969247596758 said\n",
      "-3.925018389995985 every\n",
      "flare\n",
      "3.47424257968221 shall\n",
      "1.5781694229251109 man\n",
      "1.941257592343402 unto\n",
      "2.5799658947904742 said\n",
      "0.6719167523912473 every\n",
      "flame\n",
      "-3.3459363827329778 shall\n",
      "-5.242009539490077 man\n",
      "-4.878921370071786 unto\n",
      "-4.240213067624714 said\n",
      "-6.14826221002394 every\n",
      "row\n",
      "-2.6550404372627567 shall\n",
      "-4.5511135940198555 man\n",
      "-4.188025424601564 unto\n",
      "-3.5493171221544926 said\n",
      "-5.457366264553719 every\n",
      "reel\n",
      "-0.0864723747922693 shall\n",
      "-1.982545531549368 man\n",
      "-1.6194573621310768 unto\n",
      "-0.980749059684005 said\n",
      "-2.8887982020832315 every\n",
      "industry\n",
      "-0.8037421676175555 shall\n",
      "-2.6998153243746548 man\n",
      "-2.3367271549563635 unto\n",
      "-1.6980188525092912 said\n",
      "-3.6060679949085177 every\n",
      "whirl\n",
      "0.3367390559322749 shall\n",
      "-1.5593341008248243 man\n",
      "-1.1962459314065328 unto\n",
      "-0.5575376289594608 said\n",
      "-2.4655867713586876 every\n",
      "mind\n",
      "-6.3366503322100005 shall\n",
      "-8.2327234889671 man\n",
      "-7.8696353195488085 unto\n",
      "-7.230927017101736 said\n",
      "-9.138976159500963 every\n",
      "stagger\n",
      "-0.24822344478888142 shall\n",
      "-2.1442966015459803 man\n",
      "-1.7812084321276893 unto\n",
      "-1.1425001296806172 said\n",
      "-3.0505492720798437 every\n",
      "man\n",
      "-9.308387129984935 shall\n",
      "-11.204460286742032 man\n",
      "-10.841372117323742 unto\n",
      "-10.20266381487667 said\n",
      "-12.110712957275897 every\n",
      "glow\n",
      "-1.4612171681230797 shall\n",
      "-3.3572903248801786 man\n",
      "-2.9942021554618874 unto\n",
      "-2.3554938530148153 said\n",
      "-4.263542995414042 every\n",
      "fire\n",
      "-6.695181398993863 shall\n",
      "-8.591254555750961 man\n",
      "-8.22816638633267 unto\n",
      "-7.589458083885598 said\n",
      "-9.497507226284824 every\n",
      "burn\n",
      "-4.522372135133152 shall\n",
      "-6.418445291890251 man\n",
      "-6.05535712247196 unto\n",
      "-5.4166488200248875 said\n",
      "-7.324697962424114 every\n",
      "face\n",
      "-7.098457646805082 shall\n",
      "-8.994530803562181 man\n",
      "-8.631442634143891 unto\n",
      "-7.992734331696818 said\n",
      "-9.900783474096045 every\n",
      "beam\n",
      "-1.965380557874907 shall\n",
      "-3.861453714632006 man\n",
      "-3.498365545213715 unto\n",
      "-2.859657242766643 said\n",
      "-4.767706385165869 every\n",
      "cigar\n",
      "-0.507610073607531 shall\n",
      "-2.4036832303646296 man\n",
      "-2.0405950609463384 unto\n",
      "-1.4018867584992665 said\n",
      "-3.3099359008984934 every\n",
      "skin\n",
      "-4.055578366846485 shall\n",
      "-5.9516515236035845 man\n",
      "-5.588563354185293 unto\n",
      "-4.949855051738221 said\n",
      "-6.857904194137448 every\n",
      "cigarette\n",
      "0.3699059198674743 shall\n",
      "-1.5261672368896246 man\n",
      "-1.1630790674713336 unto\n",
      "-0.5243707650242615 said\n",
      "-2.4324199074234882 every\n",
      "argument\n",
      "-2.1811092489303445 shall\n",
      "-4.077182405687443 man\n",
      "-3.714094236269152 unto\n",
      "-3.0753859338220804 said\n",
      "-4.983435076221307 every\n",
      "share\n",
      "-3.35611416773701 shall\n",
      "-5.252187324494109 man\n",
      "-4.889099155075818 unto\n",
      "-4.250390852628746 said\n",
      "-6.1584399950279725 every\n",
      "shot\n",
      "-3.317571491479616 shall\n",
      "-5.213644648236715 man\n",
      "-4.850556478818424 unto\n",
      "-4.211848176371352 said\n",
      "-6.119897318770579 every\n",
      "screen\n",
      "0.04128317240610359 shall\n",
      "-1.8547899843509952 man\n",
      "-1.491701814932704 unto\n",
      "-0.852993512485632 said\n",
      "-2.7610426548848586 every\n",
      "hope\n",
      "-6.3340781094811165 shall\n",
      "-8.230151266238215 man\n",
      "-7.8670630968199236 unto\n",
      "-7.228354794372852 said\n",
      "-9.136403936772078 every\n",
      "opinion\n",
      "-4.619571093720518 shall\n",
      "-6.515644250477617 man\n",
      "-6.152556081059326 unto\n",
      "-5.513847778612254 said\n",
      "-7.421896921011481 every\n",
      "courage\n",
      "-3.186822900124738 shall\n",
      "-5.082896056881837 man\n",
      "-4.719807887463546 unto\n",
      "-4.0810995850164735 said\n",
      "-5.9891487274157 every\n"
     ]
    }
   ],
   "source": [
    "for w in target_words:\n",
    "    \n",
    "    ppmispace = ppmi_transform(target_semantic_space, w)\n",
    "    \n",
    "    for k, v in ppmispace.items():\n",
    "        print(k)\n",
    "        for c in list(v)[:5]:\n",
    "            print(v[c], c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Dimensionality reduction is a method that does exactly this: It takes a space where each word has a vector of, say, 10,000 dimensions and reduces it to a space where each word has a vector of something like 300 or 500 dimensions, making the space more manageable.\n",
    "\n",
    "The new dimensions can be seen as groupings (soft clusterings) of the old dimensions, or as latent semantic classes underlying the old dimensions. A popular choice of dimensionality reduction method is singular value decomposition (SVD). SVD involves representing a set of points in a different space (that is, through a new set of dimensions) in such a way that it brings out the underlying structure of the data.\n",
    "\n",
    "Here is how we can do this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_to_reduce = {}\n",
    "for item in target_words:\n",
    "    space_to_reduce[item] = np.zeros(2000)\n",
    "    this_sem_space = dict(semantic_space[item])\n",
    "    for n, other_w in enumerate(dims.keys()):\n",
    "        if other_w not in this_sem_space.keys():\n",
    "            space_to_reduce[item][n] = 0\n",
    "        else:\n",
    "            space_to_reduce[item][n] = this_sem_space[other_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_transform(space, original_dim, dim_to_keep):\n",
    "    \n",
    "    # space is a dictionary mapping words to vectors\n",
    "    # combine those into a big matrix\n",
    "    spacematrix = np.empty((len(space.keys()), original_dim))\n",
    "    rowlabels = sorted(space.keys())\n",
    "\n",
    "    for index, word in enumerate(rowlabels):\n",
    "        spacematrix[index] = space[word]\n",
    "\n",
    "    # start SVD\n",
    "    umatrix, sigmavector, vmatrix = np.linalg.svd(spacematrix)\n",
    "\n",
    "    # remove the last few dimensions of u and sigma\n",
    "    utrunc = umatrix[:, :dim_to_keep]\n",
    "    sigmatrunc = sigmavector[ :dim_to_keep]\n",
    "\n",
    "    # new space: U %matrixproduct% Sigma_as_diagonal_matrix   \n",
    "    newspacematrix = np.dot(utrunc, np.diag(sigmatrunc))\n",
    "\n",
    "    # transform back to a dictionary mapping words to vectors\n",
    "    newspace = {}\n",
    "    for index, word in enumerate(rowlabels):\n",
    "        newspace[word] = newspacematrix[index]\n",
    "        \n",
    "    return newspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_space = svd_transform(space_to_reduce, 2000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-324.59862697,   93.51490343,  -34.27373355,   76.59531525,\n",
       "        176.35701049, -125.31640658, -132.07118252,   -8.28644964,\n",
       "        -47.31506928,  -23.47823178])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_space['fire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phrase_svd_space(phrase, svd_space):\n",
    "\n",
    "    subject_space = svd_space[phrase[0]]\n",
    "    verb_space = svd_space[phrase[1]]\n",
    "        \n",
    "    representation = np.zeros(len(svd_space))\n",
    "\n",
    "    #out = subject_space + verb_space\n",
    "    #out = subject_space * verb_space\n",
    "    #out = subject_space * 0.2 + verb_space * 0.8\n",
    "    out = subject_space * 0.0 + verb_space * 0.95 + (0.05 * subject_space * verb_space)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90.44279811 -5.14456791 36.35814754 -2.86711681  4.66359159  2.90481913\n",
      " 14.41698183 -0.80209509  4.09211922 -0.96749855]\n",
      "[ 1.75745363e+02  9.59732021e+00  3.82883270e+00 -3.13018079e+00\n",
      "  8.04708853e+00  1.62232226e-01  1.85807947e+01  1.05341584e+00\n",
      " -6.57953719e+00 -2.36594184e+00]\n",
      "[ 1.57255728e+03  1.02070122e+02 -1.01357118e+02  1.99985536e+01\n",
      "  2.26357612e+02  1.91022709e+02 -4.64670873e+02 -7.26842922e-01\n",
      " -1.09547613e+01 -4.56855382e+00]\n"
     ]
    }
   ],
   "source": [
    "ref = build_phrase_svd_space(reference, new_space)\n",
    "lhigh = build_phrase_svd_space(landmark_high, new_space)\n",
    "llow = build_phrase_svd_space(landmark_low, new_space)\n",
    "print(ref)\n",
    "print(lhigh)\n",
    "print(llow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9271333248815603"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(ref, lhigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8024780785051394"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(ref, llow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mitchell, J., & Lapata, M. (2008). Vector-based Models of Semantic Composition. In Proceedings of ACL-08: HLT (pp. 236–244). Association for Computational Linguistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
