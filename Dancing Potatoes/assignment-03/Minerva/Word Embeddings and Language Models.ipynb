{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Word Embeddings and Language Modelling\n",
    "\n",
    "Adam Ek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll explore constructing *static* word embeddings (i.e. word2vec) and building language models. We'll also evaluate these systems on intermediate tasks, namely word similarity and identifying \"good\" and \"bad\" sentences.\n",
    "\n",
    "* For this we'll use pytorch. Some basic operations that will be useful can be found here: https://jhui.github.io/2018/02/09/PyTorch-Basic-operations\n",
    "* In general: we are not interested in getting state-of-the-art performance :) focus on the implementation and not results of your model. For this reason, you can use a subset of the dataset: the first 5000-10 000 sentences or so, on linux/mac: ```head -n 10000 inputfile > outputfile```. \n",
    "* If possible, use the MLTGpu, it will make everything faster :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# for gpu, replace \"cpu\" with \"cuda:n\" where n is the index of the GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device('cuda:3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec embeddings\n",
    "\n",
    "In this first part we'll construct a word2vec model which will give us *static* word embeddings (that is, they are fixed after training).\n",
    "\n",
    "After we've trained our model we will evaluate the embeddings obtained on a word similarity task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load some data, you can download the file on canvas under files/03-lab-data/wiki-corpus.txt. The file contains 50 000 sentences randomly selected from the complete wikipedia. Each line in the file contains one sentence. The sentences are whitespace tokenized.\n",
    "\n",
    "Your first task is to create a dataset suitable for word2vec. That is, we define some ```window_size``` then iterate over all sentences in the dataset, putting the center word in one field and the context words in another (separate the fields with ```tab```).\n",
    "\n",
    "For example, the sentece \"this is a lab\" with ```window size = 4``` will be formatted as:\n",
    "```\n",
    "center, context\n",
    "---------------------\n",
    "this    is a lab\n",
    "is      this a lab\n",
    "a       this is lab\n",
    "lab     this is a\n",
    "```\n",
    "\n",
    "this will be our training examples when training the word2vec model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchist\\thistorian george ',\n",
       " 'historian\\tanarchist george woodcock ',\n",
       " 'george\\tanarchist historian woodcock reports ',\n",
       " 'woodcock\\thistorian george reports that ',\n",
       " 'reports\\tgeorge woodcock that \" ',\n",
       " 'that\\twoodcock reports \" the ',\n",
       " '\"\\treports that the annual ',\n",
       " 'the\\tthat \" annual congress ',\n",
       " 'annual\\t\" the congress of ',\n",
       " 'congress\\tthe annual of the ',\n",
       " 'of\\tannual congress the international ',\n",
       " 'the\\tcongress of international had ',\n",
       " 'international\\tof the had not ',\n",
       " 'had\\tthe international not taken ',\n",
       " 'not\\tinternational had taken place ',\n",
       " 'taken\\thad not place in ',\n",
       " 'place\\tnot taken in 1870 ',\n",
       " 'in\\ttaken place 1870 owing ',\n",
       " '1870\\tplace in owing to ',\n",
       " 'owing\\tin 1870 to the ',\n",
       " 'to\\t1870 owing the outbreak ',\n",
       " 'the\\towing to outbreak of ',\n",
       " 'outbreak\\tto the of the ',\n",
       " 'of\\tthe outbreak the paris ',\n",
       " 'the\\toutbreak of paris commune ',\n",
       " 'paris\\tof the commune , ',\n",
       " 'commune\\tthe paris , and ',\n",
       " ',\\tparis commune and in ',\n",
       " 'and\\tcommune , in 1871 ',\n",
       " 'in\\t, and 1871 the ',\n",
       " '1871\\tand in the general ',\n",
       " 'the\\tin 1871 general council ',\n",
       " 'general\\t1871 the council called ',\n",
       " 'council\\tthe general called only ',\n",
       " 'called\\tgeneral council only a ',\n",
       " 'only\\tcouncil called a special ',\n",
       " 'a\\tcalled only special conference ',\n",
       " 'special\\tonly a conference in ',\n",
       " 'conference\\ta special in london ',\n",
       " 'in\\tspecial conference london . ',\n",
       " 'london\\tconference in ',\n",
       " '.\\tin london ',\n",
       " 'a\\tbomb was ',\n",
       " 'bomb\\ta was thrown ',\n",
       " 'was\\ta bomb thrown by ',\n",
       " 'thrown\\tbomb was by an ',\n",
       " 'by\\twas thrown an unknown ',\n",
       " 'an\\tthrown by unknown party ',\n",
       " 'unknown\\tby an party near ',\n",
       " 'party\\tan unknown near the ',\n",
       " 'near\\tunknown party the conclusion ',\n",
       " 'the\\tparty near conclusion of ',\n",
       " 'conclusion\\tnear the of the ',\n",
       " 'of\\tthe conclusion the rally ',\n",
       " 'the\\tconclusion of rally , ',\n",
       " 'rally\\tof the , killing ',\n",
       " ',\\tthe rally killing an ',\n",
       " 'killing\\trally , an officer ',\n",
       " 'an\\t, killing officer . ',\n",
       " 'officer\\tkilling an ',\n",
       " '.\\tan officer ',\n",
       " 'in\\tthe ensuing ',\n",
       " 'the\\tin ensuing panic ',\n",
       " 'ensuing\\tin the panic , ',\n",
       " 'panic\\tthe ensuing , police ',\n",
       " ',\\tensuing panic police opened ',\n",
       " 'police\\tpanic , opened fire ',\n",
       " 'opened\\t, police fire on ',\n",
       " 'fire\\tpolice opened on the ',\n",
       " 'on\\topened fire the crowd ',\n",
       " 'the\\tfire on crowd and ',\n",
       " 'crowd\\ton the and each ',\n",
       " 'and\\tthe crowd each other ',\n",
       " 'each\\tcrowd and other . ',\n",
       " 'other\\tand each ',\n",
       " '.\\teach other ',\n",
       " 'josiah\\twarren is ',\n",
       " 'warren\\tjosiah is widely ',\n",
       " 'is\\tjosiah warren widely regarded ',\n",
       " 'widely\\twarren is regarded as ',\n",
       " 'regarded\\tis widely as the ',\n",
       " 'as\\twidely regarded the first ',\n",
       " 'the\\tregarded as first american ',\n",
       " 'first\\tas the american anarchist ',\n",
       " 'american\\tthe first anarchist , ',\n",
       " 'anarchist\\tfirst american , and ',\n",
       " ',\\tamerican anarchist and the ',\n",
       " 'and\\tanarchist , the four-page ',\n",
       " 'the\\t, and four-page weekly ',\n",
       " 'four-page\\tand the weekly paper ',\n",
       " 'weekly\\tthe four-page paper he ',\n",
       " 'paper\\tfour-page weekly he edited ',\n",
       " 'he\\tweekly paper edited during ',\n",
       " 'edited\\tpaper he during 1833 ',\n",
       " 'during\\the edited 1833 , ',\n",
       " '1833\\tedited during , the ',\n",
       " ',\\tduring 1833 the peaceful ',\n",
       " 'the\\t1833 , peaceful revolutionist ',\n",
       " 'peaceful\\t, the revolutionist , ',\n",
       " 'revolutionist\\tthe peaceful , was ',\n",
       " ',\\tpeaceful revolutionist was the ',\n",
       " 'was\\trevolutionist , the first ',\n",
       " 'the\\t, was first anarchist ',\n",
       " 'first\\twas the anarchist periodical ',\n",
       " 'anarchist\\tthe first periodical published ',\n",
       " 'periodical\\tfirst anarchist published . ',\n",
       " 'published\\tanarchist periodical ',\n",
       " '.\\tperiodical published ',\n",
       " 'weak\\tcentral coherence ',\n",
       " 'central\\tweak coherence theory ',\n",
       " 'coherence\\tweak central theory hypothesizes ',\n",
       " 'theory\\tcentral coherence hypothesizes that ',\n",
       " 'hypothesizes\\tcoherence theory that a ',\n",
       " 'that\\ttheory hypothesizes a limited ',\n",
       " 'a\\thypothesizes that limited ability ',\n",
       " 'limited\\tthat a ability to ',\n",
       " 'ability\\ta limited to see ',\n",
       " 'to\\tlimited ability see the ',\n",
       " 'see\\tability to the big ',\n",
       " 'the\\tto see big picture ',\n",
       " 'big\\tsee the picture underlies ',\n",
       " 'picture\\tthe big underlies the ',\n",
       " 'underlies\\tbig picture the central ',\n",
       " 'the\\tpicture underlies central disturbance ',\n",
       " 'central\\tunderlies the disturbance in ',\n",
       " 'disturbance\\tthe central in autism ',\n",
       " 'in\\tcentral disturbance autism . ',\n",
       " 'autism\\tdisturbance in ',\n",
       " '.\\tin autism ',\n",
       " 'the\\tword autism ',\n",
       " 'word\\tthe autism first ',\n",
       " 'autism\\tthe word first took ',\n",
       " 'first\\tword autism took its ',\n",
       " 'took\\tautism first its modern ',\n",
       " 'its\\tfirst took modern sense ',\n",
       " 'modern\\ttook its sense in ',\n",
       " 'sense\\tits modern in 1938 ',\n",
       " 'in\\tmodern sense 1938 when ',\n",
       " '1938\\tsense in when hans ',\n",
       " 'when\\tin 1938 hans asperger ',\n",
       " 'hans\\t1938 when asperger of ',\n",
       " 'asperger\\twhen hans of the ',\n",
       " 'of\\thans asperger the vienna ',\n",
       " 'the\\tasperger of vienna university ',\n",
       " 'vienna\\tof the university hospital ',\n",
       " 'university\\tthe vienna hospital adopted ',\n",
       " 'hospital\\tvienna university adopted bleuler ',\n",
       " \"adopted\\tuniversity hospital bleuler 's \",\n",
       " \"bleuler\\thospital adopted 's terminology \",\n",
       " \"'s\\tadopted bleuler terminology autistic \",\n",
       " \"terminology\\tbleuler 's autistic psychopaths \",\n",
       " \"autistic\\t's terminology psychopaths in \",\n",
       " 'psychopaths\\tterminology autistic in a ',\n",
       " 'in\\tautistic psychopaths a lecture ',\n",
       " 'a\\tpsychopaths in lecture in ',\n",
       " 'lecture\\tin a in german ',\n",
       " 'in\\ta lecture german about ',\n",
       " 'german\\tlecture in about child ',\n",
       " 'about\\tin german child psychology ',\n",
       " 'child\\tgerman about psychology . ',\n",
       " 'psychology\\tabout child ',\n",
       " '.\\tchild psychology ',\n",
       " 'most\\tland areas ',\n",
       " 'land\\tmost areas are ',\n",
       " 'areas\\tmost land are in ',\n",
       " 'are\\tland areas in an ',\n",
       " 'in\\tareas are an albedo ',\n",
       " 'an\\tare in albedo range ',\n",
       " 'albedo\\tin an range of ',\n",
       " 'range\\tan albedo of 0.1 ',\n",
       " 'of\\talbedo range 0.1 to ',\n",
       " '0.1\\trange of to 0.4 ',\n",
       " 'to\\tof 0.1 0.4 . ',\n",
       " '0.4\\t0.1 to ',\n",
       " '.\\tto 0.4 ',\n",
       " 'now\\texpanded to ',\n",
       " 'expanded\\tnow to nine ',\n",
       " 'to\\tnow expanded nine , ',\n",
       " 'nine\\texpanded to , these ',\n",
       " ',\\tto nine these include ',\n",
       " 'these\\tnine , include the ',\n",
       " 'include\\t, these the poarch ',\n",
       " 'the\\tthese include poarch band ',\n",
       " 'poarch\\tinclude the band of ',\n",
       " 'band\\tthe poarch of creek ',\n",
       " 'of\\tpoarch band creek indians ',\n",
       " 'creek\\tband of indians , ',\n",
       " 'indians\\tof creek , mowa ',\n",
       " ',\\tcreek indians mowa band ',\n",
       " 'mowa\\tindians , band of ',\n",
       " 'band\\t, mowa of choctaw ',\n",
       " 'of\\tmowa band choctaw indians ',\n",
       " 'choctaw\\tband of indians , ',\n",
       " 'indians\\tof choctaw , star ',\n",
       " ',\\tchoctaw indians star clan ',\n",
       " 'star\\tindians , clan of ',\n",
       " 'clan\\t, star of muscogee ',\n",
       " 'of\\tstar clan muscogee creeks ',\n",
       " 'muscogee\\tclan of creeks , ',\n",
       " 'creeks\\tof muscogee , echota ',\n",
       " ',\\tmuscogee creeks echota cherokee ',\n",
       " 'echota\\tcreeks , cherokee tribe ',\n",
       " 'cherokee\\t, echota tribe of ',\n",
       " 'tribe\\techota cherokee of alabama ',\n",
       " 'of\\tcherokee tribe alabama , ',\n",
       " 'alabama\\ttribe of , cherokees ',\n",
       " ',\\tof alabama cherokees of ',\n",
       " 'cherokees\\talabama , of northeast ',\n",
       " 'of\\t, cherokees northeast alabama ',\n",
       " 'northeast\\tcherokees of alabama , ',\n",
       " 'alabama\\tof northeast , cherokees ',\n",
       " ',\\tnortheast alabama cherokees of ',\n",
       " 'cherokees\\talabama , of southeast ',\n",
       " 'of\\t, cherokees southeast alabama ',\n",
       " 'southeast\\tcherokees of alabama , ',\n",
       " 'alabama\\tof southeast , ma-chis ',\n",
       " ',\\tsoutheast alabama ma-chis lower ',\n",
       " 'ma-chis\\talabama , lower creek ',\n",
       " 'lower\\t, ma-chis creek indian ',\n",
       " 'creek\\tma-chis lower indian tribe ',\n",
       " 'indian\\tlower creek tribe , ',\n",
       " 'tribe\\tcreek indian , piqua ',\n",
       " ',\\tindian tribe piqua sept ',\n",
       " 'piqua\\ttribe , sept of ',\n",
       " 'sept\\t, piqua of ohio ',\n",
       " 'of\\tpiqua sept ohio shawnee ',\n",
       " 'ohio\\tsept of shawnee tribe ',\n",
       " 'shawnee\\tof ohio tribe , ',\n",
       " 'tribe\\tohio shawnee , and ',\n",
       " ',\\tshawnee tribe and united ',\n",
       " 'and\\ttribe , united cherokee ',\n",
       " 'united\\t, and cherokee ani-yun-wiya ',\n",
       " 'cherokee\\tand united ani-yun-wiya nation ',\n",
       " 'ani-yun-wiya\\tunited cherokee nation . ',\n",
       " 'nation\\tcherokee ani-yun-wiya ',\n",
       " '.\\tani-yun-wiya nation ',\n",
       " 'of\\tthose who ',\n",
       " 'those\\tof who indicated ',\n",
       " 'who\\tof those indicated a ',\n",
       " 'indicated\\tthose who a religious ',\n",
       " 'a\\twho indicated religious preference ',\n",
       " 'religious\\tindicated a preference , ',\n",
       " 'preference\\ta religious , 59 ',\n",
       " ',\\treligious preference 59 % ',\n",
       " '59\\tpreference , % said ',\n",
       " '%\\t, 59 said they ',\n",
       " 'said\\t59 % they possessed ',\n",
       " 'they\\t% said possessed a ',\n",
       " 'possessed\\tsaid they a \" ',\n",
       " 'a\\tthey possessed \" full ',\n",
       " '\"\\tpossessed a full understanding ',\n",
       " 'full\\ta \" understanding \" ',\n",
       " 'understanding\\t\" full \" of ',\n",
       " '\"\\tfull understanding of their ',\n",
       " 'of\\tunderstanding \" their faith ',\n",
       " 'their\\t\" of faith and ',\n",
       " 'faith\\tof their and needed ',\n",
       " 'and\\ttheir faith needed no ',\n",
       " 'needed\\tfaith and no further ',\n",
       " 'no\\tand needed further learning ',\n",
       " 'further\\tneeded no learning . ',\n",
       " 'learning\\tno further ',\n",
       " '.\\tfurther learning ',\n",
       " 'once\\the realized ',\n",
       " 'he\\tonce realized that ',\n",
       " 'realized\\tonce he that his ',\n",
       " 'that\\the realized his distraction ',\n",
       " 'his\\trealized that distraction was ',\n",
       " 'distraction\\tthat his was endangering ',\n",
       " 'was\\this distraction endangering his ',\n",
       " 'endangering\\tdistraction was his life ',\n",
       " 'his\\twas endangering life , ',\n",
       " 'life\\tendangering his , he ',\n",
       " ',\\this life he refocused ',\n",
       " 'he\\tlife , refocused and ',\n",
       " 'refocused\\t, he and killed ',\n",
       " 'and\\the refocused killed her ',\n",
       " 'killed\\trefocused and her . ',\n",
       " 'her\\tand killed ',\n",
       " '.\\tkilled her ',\n",
       " 'pausanias\\thas been ',\n",
       " 'has\\tpausanias been told ',\n",
       " 'been\\tpausanias has told that ',\n",
       " 'told\\thas been that the ',\n",
       " 'that\\tbeen told the island ',\n",
       " 'the\\ttold that island is ',\n",
       " 'island\\tthat the is \" ',\n",
       " 'is\\tthe island \" covered ',\n",
       " '\"\\tisland is covered with ',\n",
       " 'covered\\tis \" with forests ',\n",
       " 'with\\t\" covered forests and ',\n",
       " 'forests\\tcovered with and full ',\n",
       " 'and\\twith forests full of ',\n",
       " 'full\\tforests and of animals ',\n",
       " 'of\\tand full animals , ',\n",
       " 'animals\\tfull of , some ',\n",
       " ',\\tof animals some wild ',\n",
       " 'some\\tanimals , wild , ',\n",
       " 'wild\\t, some , some ',\n",
       " ',\\tsome wild some tame ',\n",
       " 'some\\twild , tame . ',\n",
       " 'tame\\t, some ',\n",
       " '.\\tsome tame ',\n",
       " 'in\\tindiana , ',\n",
       " 'indiana\\tin , when ',\n",
       " ',\\tin indiana when lincoln ',\n",
       " 'when\\tindiana , lincoln was ',\n",
       " 'lincoln\\t, when was nine ',\n",
       " 'was\\twhen lincoln nine , ',\n",
       " 'nine\\tlincoln was , his ',\n",
       " ',\\twas nine his mother ',\n",
       " 'his\\tnine , mother nancy ',\n",
       " 'mother\\t, his nancy died ',\n",
       " 'nancy\\this mother died of ',\n",
       " 'died\\tmother nancy of milk ',\n",
       " 'of\\tnancy died milk sickness ',\n",
       " 'milk\\tdied of sickness in ',\n",
       " 'sickness\\tof milk in 1818 ',\n",
       " 'in\\tmilk sickness 1818 . ',\n",
       " '1818\\tsickness in ',\n",
       " '.\\tin 1818 ',\n",
       " 'canoeing\\tdown the ',\n",
       " 'down\\tcanoeing the sangamon ',\n",
       " 'the\\tcanoeing down sangamon river ',\n",
       " 'sangamon\\tdown the river , ',\n",
       " 'river\\tthe sangamon , lincoln ',\n",
       " ',\\tsangamon river lincoln ended ',\n",
       " 'lincoln\\triver , ended up ',\n",
       " 'ended\\t, lincoln up in ',\n",
       " 'up\\tlincoln ended in the ',\n",
       " 'in\\tended up the village ',\n",
       " 'the\\tup in village of ',\n",
       " 'village\\tin the of new ',\n",
       " 'of\\tthe village new salem ',\n",
       " 'new\\tvillage of salem in ',\n",
       " 'salem\\tof new in sangamon ',\n",
       " 'in\\tnew salem sangamon county ',\n",
       " 'sangamon\\tsalem in county . ',\n",
       " 'county\\tin sangamon ',\n",
       " '.\\tsangamon county ',\n",
       " \"the\\tlincolns ' \",\n",
       " \"lincolns\\tthe ' fourth \",\n",
       " \"'\\tthe lincolns fourth son \",\n",
       " \"fourth\\tlincolns ' son , \",\n",
       " \"son\\t' fourth , thomas \",\n",
       " ',\\tfourth son thomas \" ',\n",
       " 'thomas\\tson , \" tad ',\n",
       " '\"\\t, thomas tad \" ',\n",
       " 'tad\\tthomas \" \" lincoln ',\n",
       " '\"\\t\" tad lincoln , ',\n",
       " 'lincoln\\ttad \" , was ',\n",
       " ',\\t\" lincoln was born ',\n",
       " 'was\\tlincoln , born on ',\n",
       " 'born\\t, was on april ',\n",
       " 'on\\twas born april 4 ',\n",
       " 'april\\tborn on 4 , ',\n",
       " '4\\ton april , 1853 ',\n",
       " ',\\tapril 4 1853 , ',\n",
       " '1853\\t4 , , and ',\n",
       " ',\\t, 1853 and died ',\n",
       " 'and\\t1853 , died of ',\n",
       " 'died\\t, and of heart ',\n",
       " 'of\\tand died heart failure ',\n",
       " 'heart\\tdied of failure at ',\n",
       " 'failure\\tof heart at the ',\n",
       " 'at\\theart failure the age ',\n",
       " 'the\\tfailure at age of ',\n",
       " 'age\\tat the of 18 ',\n",
       " 'of\\tthe age 18 on ',\n",
       " '18\\tage of on july ',\n",
       " 'on\\tof 18 july 16 ',\n",
       " 'july\\t18 on 16 , ',\n",
       " '16\\ton july , 1871 ',\n",
       " ',\\tjuly 16 1871 . ',\n",
       " '1871\\t16 , ',\n",
       " '.\\t, 1871 ',\n",
       " 'lincoln\\talso supported ',\n",
       " 'also\\tlincoln supported the ',\n",
       " 'supported\\tlincoln also the wilmot ',\n",
       " 'the\\talso supported wilmot proviso ',\n",
       " 'wilmot\\tsupported the proviso , ',\n",
       " 'proviso\\tthe wilmot , which ',\n",
       " ',\\twilmot proviso which , ',\n",
       " 'which\\tproviso , , if ',\n",
       " ',\\t, which if it ',\n",
       " 'if\\twhich , it had ',\n",
       " 'it\\t, if had been ',\n",
       " 'had\\tif it been adopted ',\n",
       " 'been\\tit had adopted , ',\n",
       " 'adopted\\thad been , would ',\n",
       " ',\\tbeen adopted would have ',\n",
       " 'would\\tadopted , have banned ',\n",
       " 'have\\t, would banned slavery ',\n",
       " 'banned\\twould have slavery in ',\n",
       " 'slavery\\thave banned in any ',\n",
       " 'in\\tbanned slavery any u.s. ',\n",
       " 'any\\tslavery in u.s. territory ',\n",
       " 'u.s.\\tin any territory won ',\n",
       " 'territory\\tany u.s. won from ',\n",
       " 'won\\tu.s. territory from mexico ',\n",
       " 'from\\tterritory won mexico . ',\n",
       " 'mexico\\twon from ',\n",
       " '.\\tfrom mexico ',\n",
       " 'lincoln\\temphasized his ',\n",
       " 'emphasized\\tlincoln his opposition ',\n",
       " 'his\\tlincoln emphasized opposition to ',\n",
       " 'opposition\\temphasized his to polk ',\n",
       " 'to\\this opposition polk by ',\n",
       " 'polk\\topposition to by drafting ',\n",
       " 'by\\tto polk drafting and ',\n",
       " 'drafting\\tpolk by and introducing ',\n",
       " 'and\\tby drafting introducing his ',\n",
       " 'introducing\\tdrafting and his spot ',\n",
       " 'his\\tand introducing spot resolutions ',\n",
       " 'spot\\tintroducing his resolutions . ',\n",
       " 'resolutions\\this spot ',\n",
       " '.\\tspot resolutions ',\n",
       " 'realizing\\tclay was ',\n",
       " 'clay\\trealizing was unlikely ',\n",
       " 'was\\trealizing clay unlikely to ',\n",
       " 'unlikely\\tclay was to win ',\n",
       " 'to\\twas unlikely win the ',\n",
       " 'win\\tunlikely to the presidency ',\n",
       " 'the\\tto win presidency , ',\n",
       " 'presidency\\twin the , lincoln ',\n",
       " ',\\tthe presidency lincoln , ',\n",
       " 'lincoln\\tpresidency , , who ',\n",
       " ',\\t, lincoln who had ',\n",
       " 'who\\tlincoln , had pledged ',\n",
       " 'had\\t, who pledged in ',\n",
       " 'pledged\\twho had in 1846 ',\n",
       " 'in\\thad pledged 1846 to ',\n",
       " '1846\\tpledged in to serve ',\n",
       " 'to\\tin 1846 serve only ',\n",
       " 'serve\\t1846 to only one ',\n",
       " 'only\\tto serve one term ',\n",
       " 'one\\tserve only term in ',\n",
       " 'term\\tonly one in the ',\n",
       " 'in\\tone term the house ',\n",
       " 'the\\tterm in house , ',\n",
       " 'house\\tin the , supported ',\n",
       " ',\\tthe house supported general ',\n",
       " 'supported\\thouse , general zachary ',\n",
       " 'general\\t, supported zachary taylor ',\n",
       " 'zachary\\tsupported general taylor for ',\n",
       " 'taylor\\tgeneral zachary for the ',\n",
       " 'for\\tzachary taylor the whig ',\n",
       " 'the\\ttaylor for whig nomination ',\n",
       " 'whig\\tfor the nomination in ',\n",
       " 'nomination\\tthe whig in the ',\n",
       " 'in\\twhig nomination the 1848 ',\n",
       " 'the\\tnomination in 1848 presidential ',\n",
       " '1848\\tin the presidential election ',\n",
       " 'presidential\\tthe 1848 election . ',\n",
       " 'election\\t1848 presidential ',\n",
       " '.\\tpresidential election ',\n",
       " 'the\\tawards were ',\n",
       " 'awards\\tthe were first ',\n",
       " 'were\\tthe awards first given ',\n",
       " 'first\\tawards were given in ',\n",
       " 'given\\twere first in 1929 ',\n",
       " 'in\\tfirst given 1929 at ',\n",
       " '1929\\tgiven in at a ',\n",
       " 'at\\tin 1929 a ceremony ',\n",
       " 'a\\t1929 at ceremony created ',\n",
       " 'ceremony\\tat a created for ',\n",
       " 'created\\ta ceremony for the ',\n",
       " 'for\\tceremony created the awards ',\n",
       " 'the\\tcreated for awards , ',\n",
       " 'awards\\tfor the , at ',\n",
       " ',\\tthe awards at the ',\n",
       " 'at\\tawards , the hotel ',\n",
       " 'the\\t, at hotel roosevelt ',\n",
       " 'hotel\\tat the roosevelt in ',\n",
       " 'roosevelt\\tthe hotel in hollywood ',\n",
       " 'in\\thotel roosevelt hollywood . ',\n",
       " 'hollywood\\troosevelt in ',\n",
       " '.\\tin hollywood ',\n",
       " 'after\\tthe success ',\n",
       " 'the\\tafter success of ',\n",
       " 'success\\tafter the of her ',\n",
       " 'of\\tthe success her later ',\n",
       " 'her\\tsuccess of later novels ',\n",
       " 'later\\tof her novels , ',\n",
       " 'novels\\ther later , rand ',\n",
       " ',\\tlater novels rand was ',\n",
       " 'rand\\tnovels , was able ',\n",
       " 'was\\t, rand able to ',\n",
       " 'able\\trand was to release ',\n",
       " 'to\\twas able release a ',\n",
       " 'release\\table to a revised ',\n",
       " 'a\\tto release revised version ',\n",
       " 'revised\\trelease a version in ',\n",
       " 'version\\ta revised in 1959 ',\n",
       " 'in\\trevised version 1959 that ',\n",
       " '1959\\tversion in that has ',\n",
       " 'that\\tin 1959 has since ',\n",
       " 'has\\t1959 that since sold ',\n",
       " 'since\\tthat has sold over ',\n",
       " 'sold\\thas since over three ',\n",
       " 'over\\tsince sold three million ',\n",
       " 'three\\tsold over million copies ',\n",
       " 'million\\tover three copies . ',\n",
       " 'copies\\tthree million ',\n",
       " '.\\tmillion copies ',\n",
       " 'career\\tdwan operated ',\n",
       " 'dwan\\tcareer operated flying ',\n",
       " 'operated\\tcareer dwan flying a ',\n",
       " 'flying\\tdwan operated a studios ',\n",
       " 'a\\toperated flying studios in ',\n",
       " 'studios\\tflying a in la ',\n",
       " 'in\\ta studios la mesa ',\n",
       " 'la\\tstudios in mesa , ',\n",
       " 'mesa\\tin la , california ',\n",
       " ',\\tla mesa california from ',\n",
       " 'california\\tmesa , from august ',\n",
       " 'from\\t, california august , ',\n",
       " 'august\\tcalifornia from , 1911 ',\n",
       " ',\\tfrom august 1911 to ',\n",
       " '1911\\taugust , to july ',\n",
       " 'to\\t, 1911 july , ',\n",
       " 'july\\t1911 to , 1912 ',\n",
       " ',\\tto july 1912 . ',\n",
       " '1912\\tjuly , ',\n",
       " '.\\t, 1912 ',\n",
       " 'ibn\\tkhaldun provides ',\n",
       " 'khaldun\\tibn provides a ',\n",
       " 'provides\\tibn khaldun a table ',\n",
       " 'a\\tkhaldun provides table summarizing ',\n",
       " 'table\\tprovides a summarizing the ',\n",
       " 'summarizing\\ta table the zirid ',\n",
       " 'the\\ttable summarizing zirid , ',\n",
       " 'zirid\\tsummarizing the , banu ',\n",
       " ',\\tthe zirid banu ifran ',\n",
       " 'banu\\tzirid , ifran , ',\n",
       " 'ifran\\t, banu , maghrawa ',\n",
       " ',\\tbanu ifran maghrawa , ',\n",
       " 'maghrawa\\tifran , , almoravid ',\n",
       " ',\\t, maghrawa almoravid , ',\n",
       " 'almoravid\\tmaghrawa , , hammadid ',\n",
       " ',\\t, almoravid hammadid , ',\n",
       " 'hammadid\\talmoravid , , almohad ',\n",
       " ',\\t, hammadid almohad , ',\n",
       " 'almohad\\thammadid , , merinid ',\n",
       " ',\\t, almohad merinid , ',\n",
       " 'merinid\\talmohad , , abdalwadid ',\n",
       " ',\\t, merinid abdalwadid , ',\n",
       " 'abdalwadid\\tmerinid , , wattasid ',\n",
       " ',\\t, abdalwadid wattasid , ',\n",
       " 'wattasid\\tabdalwadid , , meknassa ',\n",
       " ',\\t, wattasid meknassa and ',\n",
       " 'meknassa\\twattasid , and hafsid ',\n",
       " 'and\\t, meknassa hafsid dynasties ',\n",
       " 'hafsid\\tmeknassa and dynasties . ',\n",
       " 'dynasties\\tand hafsid ',\n",
       " '.\\thafsid dynasties ',\n",
       " 'the\\tforests are ',\n",
       " 'forests\\tthe are inhabited ',\n",
       " 'are\\tthe forests inhabited by ',\n",
       " 'inhabited\\tforests are by boars ',\n",
       " 'by\\tare inhabited boars and ',\n",
       " 'boars\\tinhabited by and jackals ',\n",
       " 'and\\tby boars jackals . ',\n",
       " 'jackals\\tboars and ',\n",
       " '.\\tand jackals ',\n",
       " 'russia\\tis also ',\n",
       " 'is\\trussia also building ',\n",
       " 'also\\trussia is building two ',\n",
       " 'building\\tis also two 636-type ',\n",
       " 'two\\talso building 636-type diesel ',\n",
       " '636-type\\tbuilding two diesel submarines ',\n",
       " 'diesel\\ttwo 636-type submarines for ',\n",
       " 'submarines\\t636-type diesel for algeria ',\n",
       " 'for\\tdiesel submarines algeria . ',\n",
       " 'algeria\\tsubmarines for ',\n",
       " '.\\tfor algeria ',\n",
       " 'the\\tundisputed master ',\n",
       " 'undisputed\\tthe master of ',\n",
       " 'master\\tthe undisputed of this ',\n",
       " 'of\\tundisputed master this music ',\n",
       " 'this\\tmaster of music is ',\n",
       " 'music\\tof this is el ',\n",
       " 'is\\tthis music el hadj ',\n",
       " \"el\\tmusic is hadj m'hamed \",\n",
       " \"hadj\\tis el m'hamed el \",\n",
       " \"m'hamed\\tel hadj el anka \",\n",
       " \"el\\thadj m'hamed anka . \",\n",
       " \"anka\\tm'hamed el \",\n",
       " '.\\tel anka ',\n",
       " 'best\\tuniversities of ',\n",
       " 'universities\\tbest of qualifications ',\n",
       " 'of\\tbest universities qualifications remain ',\n",
       " 'qualifications\\tuniversities of remain the ',\n",
       " 'remain\\tof qualifications the university ',\n",
       " 'the\\tqualifications remain university of ',\n",
       " 'university\\tremain the of tlemcen ',\n",
       " 'of\\tthe university tlemcen and ',\n",
       " 'tlemcen\\tuniversity of and batna ',\n",
       " 'and\\tof tlemcen batna hadj ',\n",
       " 'batna\\ttlemcen and hadj bereket ',\n",
       " 'hadj\\tand batna bereket , ',\n",
       " 'bereket\\tbatna hadj , they ',\n",
       " ',\\thadj bereket they occupy ',\n",
       " 'they\\tbereket , occupy the ',\n",
       " 'occupy\\t, they the 26th ',\n",
       " 'the\\tthey occupy 26th and ',\n",
       " '26th\\toccupy the and 45th ',\n",
       " 'and\\tthe 26th 45th row ',\n",
       " '45th\\t26th and row in ',\n",
       " 'row\\tand 45th in africa ',\n",
       " 'in\\t45th row africa , ',\n",
       " 'africa\\trow in , which ',\n",
       " ',\\tin africa which is ',\n",
       " 'which\\tafrica , is a ',\n",
       " 'is\\t, which a very ',\n",
       " 'a\\twhich is very bad ',\n",
       " 'very\\tis a bad standing ',\n",
       " 'bad\\ta very standing . ',\n",
       " 'standing\\tvery bad ',\n",
       " '.\\tbad standing ',\n",
       " 'anthropologists\\tstudy topics ',\n",
       " 'study\\tanthropologists topics including ',\n",
       " 'topics\\tanthropologists study including the ',\n",
       " 'including\\tstudy topics the origin ',\n",
       " 'the\\ttopics including origin and ',\n",
       " 'origin\\tincluding the and evolution ',\n",
       " 'and\\tthe origin evolution of ',\n",
       " 'evolution\\torigin and of homo ',\n",
       " 'of\\tand evolution homo sapiens ',\n",
       " 'homo\\tevolution of sapiens , ',\n",
       " 'sapiens\\tof homo , the ',\n",
       " ',\\thomo sapiens the organization ',\n",
       " 'the\\tsapiens , organization of ',\n",
       " 'organization\\t, the of human ',\n",
       " 'of\\tthe organization human social ',\n",
       " 'human\\torganization of social and ',\n",
       " 'social\\tof human and cultural ',\n",
       " 'and\\thuman social cultural relations ',\n",
       " 'cultural\\tsocial and relations , ',\n",
       " 'relations\\tand cultural , human ',\n",
       " ',\\tcultural relations human physical ',\n",
       " 'human\\trelations , physical traits ',\n",
       " 'physical\\t, human traits , ',\n",
       " 'traits\\thuman physical , human ',\n",
       " ',\\tphysical traits human behavior ',\n",
       " 'human\\ttraits , behavior , ',\n",
       " 'behavior\\t, human , the ',\n",
       " ',\\thuman behavior the variations ',\n",
       " 'the\\tbehavior , variations among ',\n",
       " 'variations\\t, the among different ',\n",
       " 'among\\tthe variations different groups ',\n",
       " 'different\\tvariations among groups of ',\n",
       " 'groups\\tamong different of humans ',\n",
       " 'of\\tdifferent groups humans , ',\n",
       " 'humans\\tgroups of , how ',\n",
       " ',\\tof humans how the ',\n",
       " 'how\\thumans , the evolutionary ',\n",
       " 'the\\t, how evolutionary past ',\n",
       " 'evolutionary\\thow the past of ',\n",
       " 'past\\tthe evolutionary of homo ',\n",
       " 'of\\tevolutionary past homo sapiens ',\n",
       " 'homo\\tpast of sapiens has ',\n",
       " 'sapiens\\tof homo has influenced ',\n",
       " 'has\\thomo sapiens influenced its ',\n",
       " 'influenced\\tsapiens has its social ',\n",
       " 'its\\thas influenced social organization ',\n",
       " 'social\\tinfluenced its organization and ',\n",
       " 'organization\\tits social and culture ',\n",
       " 'and\\tsocial organization culture , ',\n",
       " 'culture\\torganization and , and ',\n",
       " ',\\tand culture and so ',\n",
       " 'and\\tculture , so forth ',\n",
       " 'so\\t, and forth . ',\n",
       " 'forth\\tand so ',\n",
       " '.\\tso forth ',\n",
       " 'the\\talchemist robert ',\n",
       " 'alchemist\\tthe robert boyle ',\n",
       " 'robert\\tthe alchemist boyle is ',\n",
       " 'boyle\\talchemist robert is credited ',\n",
       " 'is\\trobert boyle credited as ',\n",
       " 'credited\\tboyle is as being ',\n",
       " 'as\\tis credited being the ',\n",
       " 'being\\tcredited as the father ',\n",
       " 'the\\tas being father of ',\n",
       " 'father\\tbeing the of chemistry ',\n",
       " 'of\\tthe father chemistry . ',\n",
       " 'chemistry\\tfather of ',\n",
       " '.\\tof chemistry ',\n",
       " 'alchemy\\tcoexisted alongside ',\n",
       " 'coexisted\\talchemy alongside emerging ',\n",
       " 'alongside\\talchemy coexisted emerging christianity ',\n",
       " 'emerging\\tcoexisted alongside christianity . ',\n",
       " 'christianity\\talongside emerging ',\n",
       " '.\\temerging christianity ',\n",
       " 'his\\twriting portrayed ',\n",
       " 'writing\\this portrayed alchemy ',\n",
       " 'portrayed\\this writing alchemy as ',\n",
       " 'alchemy\\twriting portrayed as a ',\n",
       " 'as\\tportrayed alchemy a sort ',\n",
       " 'a\\talchemy as sort of ',\n",
       " 'sort\\tas a of terrestrial ',\n",
       " 'of\\ta sort terrestrial astronomy ',\n",
       " 'terrestrial\\tsort of astronomy in ',\n",
       " 'astronomy\\tof terrestrial in line ',\n",
       " 'in\\tterrestrial astronomy line with ',\n",
       " 'line\\tastronomy in with the ',\n",
       " 'with\\tin line the hermetic ',\n",
       " 'the\\tline with hermetic axiom ',\n",
       " 'hermetic\\twith the axiom as ',\n",
       " 'axiom\\tthe hermetic as above ',\n",
       " 'as\\thermetic axiom above so ',\n",
       " 'above\\taxiom as so below ',\n",
       " 'so\\tas above below . ',\n",
       " 'below\\tabove so ',\n",
       " '.\\tso below ',\n",
       " 'on\\tsome systems ',\n",
       " 'some\\ton systems control-s ',\n",
       " 'systems\\ton some control-s retains ',\n",
       " 'control-s\\tsome systems retains its ',\n",
       " 'retains\\tsystems control-s its meaning ',\n",
       " 'its\\tcontrol-s retains meaning but ',\n",
       " 'meaning\\tretains its but control-q ',\n",
       " 'but\\tits meaning control-q is ',\n",
       " 'control-q\\tmeaning but is replaced ',\n",
       " 'is\\tbut control-q replaced by ',\n",
       " 'replaced\\tcontrol-q is by a ',\n",
       " 'by\\tis replaced a second ',\n",
       " 'a\\treplaced by second control-s ',\n",
       " 'second\\tby a control-s to ',\n",
       " 'control-s\\ta second to resume ',\n",
       " 'to\\tsecond control-s resume output ',\n",
       " 'resume\\tcontrol-s to output . ',\n",
       " 'output\\tto resume ',\n",
       " '.\\tresume output ',\n",
       " 'her\\ttears formed ',\n",
       " 'tears\\ther formed the ',\n",
       " 'formed\\ther tears the river ',\n",
       " 'the\\ttears formed river achelous ',\n",
       " 'river\\tformed the achelous . ',\n",
       " 'achelous\\tthe river ',\n",
       " '.\\triver achelous ',\n",
       " 'there\\tare three ',\n",
       " 'are\\tthere three systems ',\n",
       " 'three\\tthere are systems of ',\n",
       " 'systems\\tare three of schools ',\n",
       " 'of\\tthree systems schools – ',\n",
       " 'schools\\tsystems of – andorran ',\n",
       " '–\\tof schools andorran , ',\n",
       " 'andorran\\tschools – , french ',\n",
       " ',\\t– andorran french and ',\n",
       " 'french\\tandorran , and spanish ',\n",
       " 'and\\t, french spanish – ',\n",
       " 'spanish\\tfrench and – which ',\n",
       " '–\\tand spanish which use ',\n",
       " 'which\\tspanish – use catalan ',\n",
       " 'use\\t– which catalan , ',\n",
       " 'catalan\\twhich use , french ',\n",
       " ',\\tuse catalan french and ',\n",
       " 'french\\tcatalan , and spanish ',\n",
       " 'and\\t, french spanish , ',\n",
       " 'spanish\\tfrench and , respectively ',\n",
       " ',\\tand spanish respectively , ',\n",
       " 'respectively\\tspanish , , as ',\n",
       " ',\\t, respectively as the ',\n",
       " 'as\\trespectively , the main ',\n",
       " 'the\\t, as main language ',\n",
       " 'main\\tas the language of ',\n",
       " 'language\\tthe main of instruction ',\n",
       " 'of\\tmain language instruction . ',\n",
       " 'instruction\\tlanguage of ',\n",
       " '.\\tof instruction ',\n",
       " 'soule\\targued that ',\n",
       " 'argued\\tsoule that the ',\n",
       " 'that\\tsoule argued the animals ',\n",
       " 'the\\targued that animals were ',\n",
       " 'animals\\tthat the were not ',\n",
       " 'were\\tthe animals not consistent ',\n",
       " 'not\\tanimals were consistent enough ',\n",
       " 'consistent\\twere not enough with ',\n",
       " 'enough\\tnot consistent with their ',\n",
       " 'with\\tconsistent enough their real ',\n",
       " 'their\\tenough with real world ',\n",
       " 'real\\twith their world inspirations ',\n",
       " 'world\\ttheir real inspirations , ',\n",
       " 'inspirations\\treal world , and ',\n",
       " ',\\tworld inspirations and said ',\n",
       " 'and\\tinspirations , said , ',\n",
       " 'said\\t, and , \" ',\n",
       " ',\\tand said \" it ',\n",
       " '\"\\tsaid , it seems ',\n",
       " 'it\\t, \" seems to ',\n",
       " 'seems\\t\" it to me ',\n",
       " 'to\\tit seems me that ',\n",
       " 'me\\tseems to that the ',\n",
       " 'that\\tto me the failure ',\n",
       " 'the\\tme that failure of ',\n",
       " 'failure\\tthat the of this ',\n",
       " 'of\\tthe failure this book ',\n",
       " 'this\\tfailure of book ( ',\n",
       " 'book\\tof this ( commercially ',\n",
       " '(\\tthis book commercially it ',\n",
       " 'commercially\\tbook ( it is ',\n",
       " 'it\\t( commercially is already ',\n",
       " 'is\\tcommercially it already assured ',\n",
       " 'already\\tit is assured of ',\n",
       " 'assured\\tis already of tremendous ',\n",
       " 'of\\talready assured tremendous success ',\n",
       " 'tremendous\\tassured of success ) ',\n",
       " 'success\\tof tremendous ) arises ',\n",
       " ')\\ttremendous success arises from ',\n",
       " 'arises\\tsuccess ) from the ',\n",
       " 'from\\t) arises the fact ',\n",
       " 'the\\tarises from fact that ',\n",
       " 'fact\\tfrom the that the ',\n",
       " 'that\\tthe fact the satire ',\n",
       " 'the\\tfact that satire deals ',\n",
       " 'satire\\tthat the deals not ',\n",
       " 'deals\\tthe satire not with ',\n",
       " 'not\\tsatire deals with something ',\n",
       " 'with\\tdeals not something the ',\n",
       " 'something\\tnot with the author ',\n",
       " 'the\\twith something author has ',\n",
       " 'author\\tsomething the has experienced ',\n",
       " 'has\\tthe author experienced , ',\n",
       " 'experienced\\tauthor has , but ',\n",
       " ',\\thas experienced but rather ',\n",
       " 'but\\texperienced , rather with ',\n",
       " 'rather\\t, but with stereotyped ',\n",
       " 'with\\tbut rather stereotyped ideas ',\n",
       " 'stereotyped\\trather with ideas about ',\n",
       " 'ideas\\twith stereotyped about a ',\n",
       " 'about\\tstereotyped ideas a country ',\n",
       " 'a\\tideas about country which ',\n",
       " 'country\\tabout a which he ',\n",
       " 'which\\ta country he probably ',\n",
       " 'he\\tcountry which probably does ',\n",
       " 'probably\\twhich he does not ',\n",
       " 'does\\the probably not know ',\n",
       " 'not\\tprobably does know very ',\n",
       " 'know\\tdoes not very well ',\n",
       " 'very\\tnot know well \" ',\n",
       " 'well\\tknow very \" . ',\n",
       " '\"\\tvery well ',\n",
       " '.\\twell \" ',\n",
       " 'orwell\\tbiographer jeffrey ',\n",
       " 'biographer\\torwell jeffrey meyers ',\n",
       " 'jeffrey\\torwell biographer meyers has ',\n",
       " 'meyers\\tbiographer jeffrey has written ',\n",
       " 'has\\tjeffrey meyers written , ',\n",
       " 'written\\tmeyers has , \" ',\n",
       " ',\\thas written \" virtually ',\n",
       " '\"\\twritten , virtually every ',\n",
       " 'virtually\\t, \" every detail ',\n",
       " 'every\\t\" virtually detail has ',\n",
       " 'detail\\tvirtually every has political ',\n",
       " 'has\\tevery detail political significance ',\n",
       " 'political\\tdetail has significance in ',\n",
       " 'significance\\thas political in this ',\n",
       " 'in\\tpolitical significance this allegory ',\n",
       " 'this\\tsignificance in allegory . ',\n",
       " 'allegory\\tin this . \" ',\n",
       " '.\\tthis allegory ',\n",
       " '\"\\tallegory . ',\n",
       " 'gymnophiona\\tthe order ',\n",
       " 'the\\tgymnophiona order gymnophiona ',\n",
       " 'order\\tgymnophiona the gymnophiona ( ',\n",
       " 'gymnophiona\\tthe order ( from ',\n",
       " '(\\torder gymnophiona from the ',\n",
       " 'from\\tgymnophiona ( the greek ',\n",
       " 'the\\t( from greek gymnos ',\n",
       " 'greek\\tfrom the gymnos meaning ',\n",
       " 'gymnos\\tthe greek meaning \" ',\n",
       " 'meaning\\tgreek gymnos \" naked ',\n",
       " '\"\\tgymnos meaning naked \" ',\n",
       " 'naked\\tmeaning \" \" and ',\n",
       " '\"\\t\" naked and ophis ',\n",
       " 'and\\tnaked \" ophis meaning ',\n",
       " 'ophis\\t\" and meaning \" ',\n",
       " 'meaning\\tand ophis \" serpent ',\n",
       " '\"\\tophis meaning serpent \" ',\n",
       " 'serpent\\tmeaning \" \" ) ',\n",
       " '\"\\t\" serpent ) or ',\n",
       " ')\\tserpent \" or apoda ',\n",
       " 'or\\t\" ) apoda ( ',\n",
       " 'apoda\\t) or ( from ',\n",
       " '(\\tor apoda from the ',\n",
       " 'from\\tapoda ( the latin ',\n",
       " 'the\\t( from latin an ',\n",
       " 'latin\\tfrom the an - ',\n",
       " 'an\\tthe latin - meaning ',\n",
       " '-\\tlatin an meaning \" ',\n",
       " 'meaning\\tan - \" without ',\n",
       " '\"\\t- meaning without \" ',\n",
       " 'without\\tmeaning \" \" and ',\n",
       " '\"\\t\" without and the ',\n",
       " 'and\\twithout \" the greek ',\n",
       " 'the\\t\" and greek poda ',\n",
       " 'greek\\tand the poda meaning ',\n",
       " 'poda\\tthe greek meaning \" ',\n",
       " 'meaning\\tgreek poda \" legs ',\n",
       " '\"\\tpoda meaning legs \" ',\n",
       " 'legs\\tmeaning \" \" ) ',\n",
       " '\"\\t\" legs ) comprise ',\n",
       " ')\\tlegs \" comprise the ',\n",
       " 'comprise\\t\" ) the caecilians ',\n",
       " 'the\\t) comprise caecilians . ',\n",
       " 'caecilians\\tcomprise the ',\n",
       " '.\\tthe caecilians ',\n",
       " 'other\\tsatellite males ',\n",
       " 'satellite\\tother males remain ',\n",
       " 'males\\tother satellite remain quietly ',\n",
       " 'remain\\tsatellite males quietly nearby ',\n",
       " 'quietly\\tmales remain nearby , ',\n",
       " 'nearby\\tremain quietly , waiting ',\n",
       " ',\\tquietly nearby waiting for ',\n",
       " 'waiting\\tnearby , for their ',\n",
       " 'for\\t, waiting their opportunity ',\n",
       " 'their\\twaiting for opportunity to ',\n",
       " 'opportunity\\tfor their to take ',\n",
       " 'to\\ttheir opportunity take over ',\n",
       " 'take\\topportunity to over a ',\n",
       " 'over\\tto take a territory ',\n",
       " 'a\\ttake over territory . ',\n",
       " 'territory\\tover a ',\n",
       " '.\\ta territory ',\n",
       " 'meanwhile\\tthey have ',\n",
       " 'they\\tmeanwhile have been ',\n",
       " 'have\\tmeanwhile they been observed ',\n",
       " 'been\\tthey have observed to ',\n",
       " 'observed\\thave been to ingest ',\n",
       " 'to\\tbeen observed ingest fluid ',\n",
       " 'ingest\\tobserved to fluid exuded ',\n",
       " 'fluid\\tto ingest exuded from ',\n",
       " 'exuded\\tingest fluid from the ',\n",
       " 'from\\tfluid exuded the maternal ',\n",
       " 'the\\texuded from maternal cloaca ',\n",
       " 'maternal\\tfrom the cloaca . ',\n",
       " 'cloaca\\tthe maternal ',\n",
       " '.\\tmaternal cloaca ',\n",
       " 'these\\tare part ',\n",
       " 'are\\tthese part of ',\n",
       " 'part\\tthese are of the ',\n",
       " 'of\\tare part the broader ',\n",
       " 'the\\tpart of broader discipline ',\n",
       " 'broader\\tof the discipline of ',\n",
       " 'discipline\\tthe broader of zoology ',\n",
       " 'of\\tbroader discipline zoology but ',\n",
       " 'zoology\\tdiscipline of but are ',\n",
       " 'but\\tof zoology are seldom ',\n",
       " 'are\\tzoology but seldom studied ',\n",
       " 'seldom\\tbut are studied alone ',\n",
       " 'studied\\tare seldom alone . ',\n",
       " 'alone\\tseldom studied ',\n",
       " '.\\tstudied alone ',\n",
       " 'the\\tstate has ',\n",
       " 'state\\tthe has an ',\n",
       " 'has\\tthe state an independence ',\n",
       " 'an\\tstate has independence movement ',\n",
       " 'independence\\thas an movement favoring ',\n",
       " 'movement\\tan independence favoring a ',\n",
       " 'favoring\\tindependence movement a vote ',\n",
       " 'a\\tmovement favoring vote on ',\n",
       " 'vote\\tfavoring a on secession ',\n",
       " 'on\\ta vote secession from ',\n",
       " 'secession\\tvote on from the ',\n",
       " 'from\\ton secession the united ',\n",
       " 'the\\tsecession from united states ',\n",
       " 'united\\tfrom the states , ',\n",
       " 'states\\tthe united , with ',\n",
       " ',\\tunited states with the ',\n",
       " 'with\\tstates , the alaskan ',\n",
       " 'the\\t, with alaskan independence ',\n",
       " 'alaskan\\twith the independence party ',\n",
       " 'independence\\tthe alaskan party . ',\n",
       " 'party\\talaskan independence ',\n",
       " '.\\tindependence party ',\n",
       " 'between\\t1700 and ',\n",
       " '1700\\tbetween and 1980 ',\n",
       " 'and\\tbetween 1700 1980 , ',\n",
       " '1980\\t1700 and , \" ',\n",
       " ',\\tand 1980 \" the ',\n",
       " '\"\\t1980 , the total ',\n",
       " 'the\\t, \" total area ',\n",
       " 'total\\t\" the area of ',\n",
       " 'area\\tthe total of cultivated ',\n",
       " 'of\\ttotal area cultivated land ',\n",
       " 'cultivated\\tarea of land worldwide ',\n",
       " 'land\\tof cultivated worldwide increased ',\n",
       " 'worldwide\\tcultivated land increased 466 ',\n",
       " 'increased\\tland worldwide 466 % ',\n",
       " '466\\tworldwide increased % \" ',\n",
       " '%\\tincreased 466 \" and ',\n",
       " '\"\\t466 % and yields ',\n",
       " 'and\\t% \" yields increased ',\n",
       " 'yields\\t\" and increased dramatically ',\n",
       " 'increased\\tand yields dramatically , ',\n",
       " 'dramatically\\tyields increased , particularly ',\n",
       " ',\\tincreased dramatically particularly because ',\n",
       " 'particularly\\tdramatically , because of ',\n",
       " 'because\\t, particularly of selectively ',\n",
       " ...]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'wiki-short.txt'\n",
    "WINDOW_SIZE = 4\n",
    "def corpus_reader(data_path):\n",
    "    with open(data_path) as f:\n",
    "        #dataset = {} # final result looks like this: { key (sentence): {key (word): [context]} }\n",
    "        datalist = []\n",
    "        \n",
    "        # handle window size being odd\n",
    "        if WINDOW_SIZE % 2 == 0:\n",
    "            divided = WINDOW_SIZE // 2\n",
    "            window_size_split = (divided, divided)\n",
    "        else:\n",
    "            lmd = lambda n: (n//2+1, n//2)\n",
    "            window_size_split = lmd(WINDOW_SIZE)\n",
    "                \n",
    "        for line in f:\n",
    "            #dataset[line] = {}\n",
    "            tokens = line.split()\n",
    "            # TODO I suppose we should remove punctuation etc?\n",
    "            for i, t in enumerate(tokens):\n",
    "                #context = []\n",
    "                context = \"\"\n",
    "                \n",
    "                # handle tokens at the start and beginning by taking more next/prev tokens\n",
    "                next_window = window_size_split[0]\n",
    "                prev_window = window_size_split[1]\n",
    "                \n",
    "                # if there are no/less than prev_window tokens\n",
    "                #if i - prev_window < 0:\n",
    "                #    next_window += (i - prev_window)*-1\n",
    "                #    prev_window += i - prev_window\n",
    "                # same fot next_window tokens\n",
    "                #elif i+next_window > len(tokens) - 1:\n",
    "                #    prev_window += (i+next_window) - (len(tokens) - 1)\n",
    "                #    next_window -= (i+next_window) - (len(tokens) - 1)\n",
    "                    \n",
    "                # add previous tokens\n",
    "                prev_num = i-prev_window\n",
    "                for ind in range(prev_num, i):\n",
    "                    if ind > -1 and len(tokens) > ind:\n",
    "                        #context.append(tokens[ind])\n",
    "                        context = context + tokens[ind].lower() + \" \"\n",
    "                        \n",
    "                # add next tokens\n",
    "                next_num = i+next_window+1\n",
    "                for ind in range (i+1, next_num):\n",
    "                    if next_num <= len(tokens):\n",
    "                        #context.append(tokens[ind].lower())\n",
    "                        context = context + tokens[ind].lower() + \" \"\n",
    "\n",
    "                #datalist.append([t.lower(), context])\n",
    "                if len(context) > 0:\n",
    "                    datalist.append(t.lower() + \"\\t\" + context)\n",
    "                #dataset[line][t] = context # TODO if word already exists (line \"in\" in the first sentence)\n",
    "        \n",
    "        fields = ['center', 'context']\n",
    "    \n",
    "        with open('train.csv', 'w') as f:\n",
    "            f.write(\"center\\tcontext\"+\"\\n\")\n",
    "            for line in datalist:\n",
    "                if line.strip():\n",
    "                    f.write(line+\"\\n\")\n",
    "    \n",
    "        return datalist#dataset\n",
    "\n",
    "corpus_reader(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sampled 50 000 senteces completely random from the *whole* wikipedia for our training data. Give some reasons why this is good, and why it might be bad. (*note*: We'll have a few questions like these, one or two reasons for and against is sufficient)\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good: a larger variety ?\n",
    "Bad: \n",
    "\n",
    "TODOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We now need to load the data in an appropriate format for torchtext (https://torchtext.readthedocs.io/en/latest/). We'll use PyText for this and it'll follow the same structure as I showed you in the lecture (remember to lower-case all tokens). Create a function which returns a (bucket)iterator of the training data, and the vocabulary object (```Field```). \n",
    "\n",
    "(*hint1*: you can format the data such that the center word always is first, then you only need to use one field)\n",
    "\n",
    "(*hint2*: the code I showed you during the leture is available in /files/pytorch_tutorial/ on canvas)\n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import csv \n",
    "\n",
    "def get_data():\n",
    "    ddir = '.'\n",
    "    whitespacer = lambda x: x.split(' ')\n",
    "\n",
    "    # \"fields\" that process the different columns in our CSV files\n",
    "    CENTER = Field(tokenize    = whitespacer,\n",
    "                   lower       = True,\n",
    "                   batch_first = True) # enforce the (batch, words) structure\n",
    "\n",
    "    CONTEXT = Field(tokenize    = whitespacer,\n",
    "                   batch_first = True)\n",
    "\n",
    "    # read the csv files\n",
    "    train = TabularDataset('train.csv', \"csv\", \n",
    "                           [('center', CENTER),\n",
    "                            ('context', CONTEXT)], \n",
    "                           skip_header=True, \n",
    "                           csv_reader_params = {'delimiter':'\\t',\n",
    "                                                'quotechar':'Ö'})\n",
    "\n",
    "    # build vocabularies based on what our csv files contained and create word2id mapping\n",
    "    CENTER.build_vocab(train)\n",
    "    CONTEXT.build_vocab(train)\n",
    "    \n",
    "    NEW = Field(tokenize    = whitespacer,\n",
    "                batch_first = True)\n",
    "    NEW.build_vocab(train.center, train.context)\n",
    "    \n",
    "    print(len(CENTER.vocab))\n",
    "    print(len(CONTEXT.vocab))\n",
    "    print(len(NEW.vocab))\n",
    "\n",
    "    # create batches from our data, and shuffle them for each epoch\n",
    "    train_iter = BucketIterator(train,\n",
    "                                                  batch_size        = 8,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.center),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "\n",
    "    return train_iter, CENTER, CONTEXT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31228\n",
      "31232\n",
      "31232\n"
     ]
    }
   ],
   "source": [
    "a,b,c = get_data()\n",
    "#x = next(iter(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NEW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-de4530286838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNEW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'NEW' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lower-cased all tokens above; give some reasons why this is a good idea, and why it may be harmful to our embeddings.\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good:\n",
    "Harmful: cant recognise NE's?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the CBOW model for constructing word embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CBOW model we try to predict the center word based on the context. That is, we take as input ```n``` context words, encode them as vectors, then combine them by summation. This will give us one embedding. We then use this embedding to predict *which* word in our vocabuary is the most likely center word. \n",
    "\n",
    "Implement this model \n",
    "\n",
    "[7 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_len, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_len, embedding_dim)\n",
    "        self.prediction = nn.Linear(embedding_dim, vocab_len) # IDK\n",
    "    \n",
    "    def forward(self, context):\n",
    "        embedded_context = self.embeddings(context)\n",
    "        xs = torch.tensor(embedded_context) \n",
    "        # create tensor of size (batch_size,window_size,embedding_dim)\n",
    "        # batch_size elements of window_size elements of embedding_dim elements so\n",
    "        # the OUTER most  \"list\" is of lenght batch_size, lets say 2 for simplicity now (each list has 2 elements)\n",
    "        # each of those 2 lists are of window_size, lets say 3, length (each list has 3 elements)\n",
    "        # and each of THOSE inner lists is of lenght embedding_dim, and this final list has numbers as values\n",
    "        # lets say embedding_dim is 4\n",
    "        #[\n",
    "        #[[1 2 3 4],[5 6 7 8],[9 1 2 3]],\n",
    "        #[[4 5 6 7],[8 9 1 2],[3 4 5 6]]\n",
    "        #]\n",
    "        # and the innermost elements are... what? Also, not sure how to create this (if it is even the correct idea?)\n",
    "        projection = self.projection_function(xs)\n",
    "        predictions = self.prediction(projection)\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    def projection_function(self, xs):\n",
    "        \"\"\"\n",
    "        This function will take as input a tensor of size (B, S, D)\n",
    "        where B is the batch_size, S the window size, and D the dimensionality of embeddings\n",
    "        this function should compute the sum over the embedding dimensions of the input, \n",
    "        that is, we transform (B, S, D) to (B, 1, D) or (B, D) \n",
    "        \"\"\"\n",
    "        xs_sum =  xs.unsqueeze(0) #???#xs.reshape()\n",
    "        return xs_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train the models. First we define which hyperparameters to use. (You can change these, for example when *developing* your model you can use a batch size of 2 and a very low dimensionality (say 10), just to speed things up). When actually training your model *fo real*, you can use a batch size of [8,16,32,64], and embedding dimensionality of [128,256]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs :)\n",
    "word_embeddings_hyperparameters = {'epochs':3,\n",
    "                                   'batch_size':2,#16,\n",
    "                                   'embedding_size':128,\n",
    "                                   'learning_rate':0.001,\n",
    "                                   'embedding_dim':10}#128}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model. Iterate over the dataset, get outputs from your model, calculate loss and backpropagate.\n",
    "\n",
    "We mentioned in the lecture that we use Negative Log Likelihood (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss to train Word2Vec model. In this lab we'll take a shortcut when *training* and use Cross Entropy Loss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), basically it combines ```log_softmax``` and ```NLLLoss```. So what your model should output is a *score* for each word in our vocabulary. The ```CrossEntropyLoss``` will then assign probabilities and calculate the negative log likelihood loss.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31228\n",
      "31232\n",
      "31232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8]' is invalid for input of size 1249120",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5f90376c82f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# send your batch of sentences to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbow_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[8]' is invalid for input of size 1249120"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset, vocab, v2 = get_data()\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "cbow_model = CBOWModel(len(vocab.vocab), word_embeddings_hyperparameters['embedding_dim'])\n",
    "cbow_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['learning_rate'])\n",
    "\n",
    "# start training loop\n",
    "cbow_model.train()\n",
    "total_loss = 0\n",
    "for epoch in range(word_embeddings_hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(dataset):\n",
    "        context = batch.context\n",
    "        target_word = batch.center\n",
    "        \n",
    "        # send your batch of sentences to the model\n",
    "        output = cbow_model(context)\n",
    "        #print(output.reshape(len(target_word)))\n",
    "        #print(target_word)\n",
    "        \n",
    "        # compute the loss, you'll need to reshape the input\n",
    "        # you can read more about this is the documentation for\n",
    "        # CrossEntropyLoss\n",
    "        # TODO currently stuck at this part > reshape output.view(-1,len(vocab.vocab)) from len 40 to len 8\n",
    "        loss = loss_fn(output.view(-1,len(target_word)), target_word.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # print average loss for the epoch\n",
    "        print(total_loss/(i+1), end='\\r') \n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the model on a dataset of word similarities, WordSim353 (http://alfonseca.org/eng/research/wordsim353.html , also avalable in vanvas under files/03-l). The first thing we need to do is read the dataset and translate it to integers. What we'll do is to reuse the ```Field``` that records word indexes (the second output of ```get_data()```) and use it to parse the file.\n",
    "\n",
    "The wordsim data is structured as follows:\n",
    "\n",
    "```\n",
    "word1 word2 score\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "The ```Field``` we got from ```read_data()``` has two built-in functions, ```stoi``` which maps a string to an integer and ```itos``` which maps an integer to a string. \n",
    "\n",
    "What our datareader needs to do is: \n",
    "\n",
    "```\n",
    "for line in file:\n",
    "    word1, word2, score = file.split()\n",
    "    # encode word1 and word2 as integers\n",
    "    word1_idx = vocab.vocab.stoi[word1]\n",
    "    word2_idx = vocab.vocab.stoi[word2]\n",
    "```\n",
    "\n",
    "when we have the integers for ```word_1``` and ```word2``` we'll compute the similarity between their word embeddings with *cosine simlarity*. We can obtain the embeddings by querying the embedding layer of the model.\n",
    "\n",
    "We calculate the cosine similarity for each word pair in the dataset, then compute the pearson correlation between the similarities we obtained with the scores given in the dataset. \n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "def read_wordsim(path, vocab, embeddings):\n",
    "    dataset_sims = []\n",
    "    model_sims = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            word1, word2, score = f.split()\n",
    "            \n",
    "            score = float(score)\n",
    "            dataset_sims.append(score)\n",
    "            \n",
    "            # get the index for the word\n",
    "            word1_idx = ...\n",
    "            word2_idx = ...\n",
    "            \n",
    "            # get the embedding of the word\n",
    "            word1_emb = ...\n",
    "            word2_emb = ...\n",
    "            \n",
    "            # compute cosine similarity, we'll use the version included in pytorch functional\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.functional.cosine_similarity.html\n",
    "            cosine_similarity = F.cosine_similarity(...)\n",
    "            \n",
    "            model_sims.append(cosine_similarity.item())\n",
    "    \n",
    "    return dataset_sims, model_sims\n",
    "\n",
    "path = 'wordsim_similarity_goldstandard.txt'\n",
    "data, model = read_wordsim(...)\n",
    "pearson_correlation = np.corrcoef(data, model)\n",
    "            \n",
    "# the non-diagonals give the pearson correlation,\n",
    "print(pearson_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the model performs good or bad? Why?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the 10 best and 10 worst performing word pairs, can you see any patterns that explain why *these* are the best and worst word pairs?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways of improving the model we apply to WordSim353.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider a scenario where we use these embeddings in a downstream task, for example sentiment analysis (roughly: determining whether a sentence is positive or negative). \n",
    "\n",
    "Give some examples why the sentiment analysis model would benefit from our embeddnings and one examples why our embeddings could hur the performance of the sentiment model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part we'll build a simple LSTM language model. Your task is to construct a model which takes a sentence as input and predict the next word for each word in the sentence. For this you'll use the ```LSTM``` class provided by PyTorch (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). You can read more about the LSTM here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "NOTE!!!: Use the same dataset (wiki-cropus.txt) as before.\n",
    "\n",
    "Our setup is similar to before, we first encode the words as distributed representations then pass these to the LSTM and for each output we predict the next word.\n",
    "\n",
    "For this we'll build a new dataloader with torchtext, the file we pass to the dataloader should contain one sentence per line, with words separated by whitespace.\n",
    "\n",
    "```\n",
    "word_1, ..., word_n\n",
    "word_1, ..., word_k\n",
    "...\n",
    "```\n",
    "\n",
    "in this dataloader you want to make sure that each sentence begins with a ```<start>``` token and ends with a ```<end>``` token, there is a keyword argument in ```Field``` for this :). But other than that, as before you read the dataset and output a iterator over the dataset and a vocabulary. \n",
    "\n",
    "Implement the dataloader, language model and the training loop (the training loop will basically be the same as for word2vec).\n",
    "\n",
    "[12 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs as before :)\n",
    "lm_hyperparameters = {'epochs':3,\n",
    "                      'batch_size':16,\n",
    "                      'learning_rate':0.001,\n",
    "                      'embedding_dim':128,\n",
    "                      'output_dim':128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'wiki-corpus.txt'\n",
    "def get_data():\n",
    "    # your code here, roughly the same as for the word2vec dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_withLSTM(nn.Module):\n",
    "    def __init__(...):\n",
    "        super(LM_withLSTM, self).__init__()\n",
    "        self.embeddings = ...\n",
    "        self.LSTM = ...\n",
    "        self.predict_word = ...\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        embedded_seq = ...\n",
    "        timestep_reprentation, *_ = ...\n",
    "        predicted_words = ...\n",
    "        \n",
    "        return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset, vocab = get_data(...)\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "lm_model = LM_withLSTM(len(vocab), \n",
    "                       lm_hyperparameters['embedding_dim'],\n",
    "                       lm_hyperparameters['output_dim'])\n",
    "lm_model.to(device)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=lm_hyperparameters['lr'])\n",
    "\n",
    "# start training loop\n",
    "total_loss = 0\n",
    "for epoch in range(lm_hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(dataset):\n",
    "        \n",
    "        # the strucure for each BATCH is:\n",
    "        # <start>, w0, ..., wn, <end>\n",
    "        sentence = batch.sentence\n",
    "        \n",
    "        # when training the model, at each input we predict the *NEXT* token\n",
    "        # consequently there is nothing to predict when we give the model \n",
    "        # <end> as input. \n",
    "        # thus, we do not want to give <end> as input to the model, select \n",
    "        # from each batch all tokens except the last. \n",
    "        # tip: use pytorch indexing/slicing (same as numpy) \n",
    "        # (https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors)\n",
    "        # (https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/)\n",
    "        input_sentence = ...\n",
    "        \n",
    "        # send your batch of sentences to the model\n",
    "        output = lm_model(input_sentence)\n",
    "        \n",
    "        # for each output, the model predict the NEXT token, so we have to reshape \n",
    "        # our dataset again. On timestep t, we evaluate on token t+1. That is,\n",
    "        # we never predict the <start> token ;) so this time, we select all but the first \n",
    "        # token from sentences (that is, all the tokens that we predict)\n",
    "        gold_data = ...\n",
    "        \n",
    "        # the shape of the output and sentence variable need to be changed,\n",
    "        # for the loss function. Details are in the documentation.\n",
    "        # You can use .view(...,...) to reshape the tensors  \n",
    "        loss = loss_fn(...)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # print average loss for the epoch\n",
    "        print(total_loss/(i+1), end='\\r') \n",
    "        \n",
    "        # compute gradients\n",
    "        ...\n",
    "        \n",
    "        # update parameters\n",
    "        ...\n",
    "        \n",
    "        # reset gradients\n",
    "        ...\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the language model\n",
    "\n",
    "We'll evaluate our model using the BLiMP dataset (https://github.com/alexwarstadt/blimp). The BLiMP dataset contains sets of linguistic minimal pairs for various syntactic and semantic phenomena, We'll evaluate our model on *existential quantifiers* (link: https://github.com/alexwarstadt/blimp/blob/master/data/existential_there_quantifiers_1.jsonl). This data, as the name suggests, investigate whether language models assign higher probability to *correct* usage of there-quantifiers. \n",
    "\n",
    "An example entry in the dataset is: \n",
    "\n",
    "```\n",
    "{\"sentence_good\": \"There was a documentary about music irritating Allison.\", \"sentence_bad\": \"There was each documentary about music irritating Allison.\", \"field\": \"semantics\", \"linguistics_term\": \"quantifiers\", \"UID\": \"existential_there_quantifiers_1\", \"simple_LM_method\": true, \"one_prefix_method\": false, \"two_prefix_method\": false, \"lexically_identical\": false, \"pairID\": \"0\"}\n",
    "```\n",
    "\n",
    "Download the dataset and build a datareader (similar to what you did for word2vec). The dataset structure you should aim for is (you don't need to worry about the other keys for this assignment):\n",
    "\n",
    "```\n",
    "good_sentence_1, bad_sentence_1\n",
    "...\n",
    "```\n",
    "\n",
    "your task now is to compare the probability assigned to the good sentence with to the probability assigned to the bad sentence. To compute a probability for a sentence we consider the product of the probabilities assigned to the *gold* tokens, remember, at timestep ```t``` we're predicting which token comes *next* e.g. ```t+1``` (basically, you do the same thing as you did when training).\n",
    "\n",
    "In rough pseudo code what your code should do is:\n",
    "\n",
    "```\n",
    "accuracy = []\n",
    "for good_sentence, bad_sentence in dataset:\n",
    "    gs_lm_output = LanguageModel(good_sentence)\n",
    "    gs_token_probabilities = softmax(gs_lm_output)\n",
    "    gs_sentence_probability = product(gs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    bs_lm_output = LanguageModel(bad_sentence)\n",
    "    bs_token_probabilities = softmax(bs_lm_output)\n",
    "    bs_sentence_probability = product(bs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    # int(True) = 1 and int(False) = 0\n",
    "    is_correct = int(gs_sentence_probability > bs_sentence_probability)\n",
    "    accuracy.append(is_correct)\n",
    "\n",
    "print(numpy.mean(accuracy))\n",
    "    \n",
    "```\n",
    "\n",
    "[6 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "import json\n",
    "\n",
    "def evaluate_model(path, vocab, model):\n",
    "    \n",
    "    accuracy = []\n",
    "    with open(path) as f:\n",
    "        # iterate over one pair of sentences at a time\n",
    "        for line in f:\n",
    "            # load the data\n",
    "            data = json.loads(line)\n",
    "            good_s = data['sentence_good']\n",
    "            bad_s = data['sentence_bad']\n",
    "            \n",
    "            # the data is tokenized as whitespace\n",
    "            tok_good_s = ...\n",
    "            tok_bad_s = ...\n",
    "            \n",
    "            # encode your words as integers using the vocab from the dataloader, size is (S)\n",
    "            # we use unsqueeze to create the batch dimension \n",
    "            # in this case our input is only ONE batch, so the size of the tensor becomes: \n",
    "            # (S) -> (1, S) as the model expects batches\n",
    "            enc_good_s = torch.tensor([_ for x in tok_good_s], device=device).unsqueeze(0)\n",
    "            enc_bad_s = torch.tensor([_ for x in tok_bad_s], device=device).unsqueeze(0)\n",
    "            \n",
    "            # pass your encoded sentences to the model and predict the next tokens\n",
    "            good_s = LM_withLSTM(enc_good_s)\n",
    "            bad_s = LM_withLSTM(enc_bad_s)\n",
    "            \n",
    "            # get probabilities with softmax\n",
    "            gs_probs = F.softmax(...)\n",
    "            bs_probs = F.softmax(...)\n",
    "            \n",
    "            # select the probability of the gold tokens\n",
    "            gs_sent_prob = find_token_probs(gs_probs, enc_good_s)\n",
    "            bs_sent_prob = find_token_probs(bs_probs, enc_bad_s)\n",
    "            \n",
    "            accuracy.append(int(gs_sent_prob>bs_sent_prob))\n",
    "            \n",
    "    return accuracy\n",
    "            \n",
    "def find_token_probs(model_probs, encoded_sentece):\n",
    "    probs = []\n",
    "\n",
    "    # iterate over the tokens in your encoded sentence\n",
    "    for token, gold_token in enumerate(encoded_sentece):\n",
    "        # select the probability of the gold tokens and save\n",
    "        # hint: pytorch indexing is helpful here ;)\n",
    "        prob = ...\n",
    "        probs.append(prob)\n",
    "    sentence_prob = ...\n",
    "    return sentence_prob\n",
    "\n",
    "path = 'existential_there_quantifiers_1.jsonl'\n",
    "accuracy = evaluate_model(path, ..., ...)\n",
    "\n",
    "print('Final accuracy:')\n",
    "print(np.round(np.mean(accuracy), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model get some score, say, 55% correct predictions. Is this good? Suggest some *baseline* (i.e. a stupid \"model\" we hope ours is better than) we can compare the model against.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some improvements you could make to your language model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some other metrics we can use to evaluate our system\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "\n",
    "Neural architectures:\n",
    "* Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. (Links to an external site.) Journal of Machine Learning Research, 3(6):1137–1155, 2003. (Sections 3 and 4 are less relevant today and hence you can glance through them quickly. Instead, look at the Mikolov papers where they describe training word embeddings with the current neural network architectures.)\n",
    "* T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
    "* T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total marks: 63"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
